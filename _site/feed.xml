<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-12-17T22:54:24+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">JSC’s DevLog</title><subtitle>SE and AI version 1.0</subtitle><author><name>JSC</name><email>newwin0198@handong.ac.kr</email></author><entry><title type="html">2023-HGU-ML Lecture 3. Probability and Statistics for ML</title><link href="http://localhost:4000/recent/Probability_and_Statistics/" rel="alternate" type="text/html" title="2023-HGU-ML Lecture 3. Probability and Statistics for ML" /><published>2023-12-17T00:00:00+09:00</published><updated>2023-12-17T00:00:00+09:00</updated><id>http://localhost:4000/recent/Probability_and_Statistics</id><content type="html" xml:base="http://localhost:4000/recent/Probability_and_Statistics/"><![CDATA[<h1 id="probability-and-statistics-for-ml">Probability and Statistics for ML</h1>

<ul>
  <li>Probability
    <ul>
      <li>IID: independent identically distributed</li>
      <li>Simpson’s Paradox</li>
      <li>Probability is a number assigned to an event indicating “how likely” the event will occur when randomly selected</li>
      <li>uncertainty can make the model simpler. A simple/uncertain model rather than a complex/certain one, even when modeling the true/deterministic rule is possible</li>
      <li>mathematical probability measures the likelihood that events will occur</li>
      <li>empirical statistics is science of collecting and analyzing numerical data in large quantities</li>
      <li>Random Variable
        <ul>
          <li>a variable whose possible values are numerical outcomes of a random phenomenon</li>
          <li>is a measurable function from a set of possible outcomes to a measurable space</li>
        </ul>
      </li>
    </ul>

    <p><img src="../../../assets/ML/ProbAndStats/Untitled.png" /></p>

    <p><img src="../../../assets/ML/ProbAndStats/Untitled 1.png" /></p>

    <ul>
      <li>marginal, joint, and conditional probabilities</li>
    </ul>

    <p><img src="../../../assets/ML/ProbAndStats/Untitled 2.png" /></p>

    <p><img src="../../../assets/ML/ProbAndStats/Untitled 3.png" /></p>

    <p><img src="../../../assets/ML/ProbAndStats/Untitled 4.png" /></p>

    <ul>
      <li>Bayes rule presents the transformation process of our beliefs.
        <ul>
          <li>subjective belief on Y changes with X</li>
        </ul>
      </li>
      <li>learning is
        <ul>
          <li>nothing but to update our confidence from prior to posterior.</li>
          <li>the key factor is likelihood which is based on data.</li>
        </ul>
      </li>
      <li>
        <p>Bayesianism (Thomas Bayes) vs. frequentism (Ronald A. Fisher)</p>

        <p>Bayesianism and frequentism are two distinct approaches to statistical inference, and they differ in their fundamental principles and interpretations of probability. Thomas Bayes and Ronald A. Fisher were prominent figures associated with these respective approaches.</p>

        <p>### Bayesianism (Thomas Bayes):</p>

        <ol>
          <li><strong>Probability as Belief:</strong>
            <ul>
              <li>In Bayesian statistics, probability is interpreted as a measure of belief or confidence in a particular hypothesis. It reflects the degree of certainty or uncertainty given available information.</li>
              <li>Bayesians start with a prior probability distribution, representing initial beliefs about a hypothesis before observing data.</li>
            </ul>
          </li>
          <li><strong>Bayes’ Theorem:</strong>
            <ul>
              <li>The cornerstone of Bayesian statistics is Bayes’ theorem, which updates the prior belief based on observed data to obtain a posterior probability distribution.</li>
              <li>Bayes’ theorem relates the posterior probability (probability of the hypothesis given the data) to the prior probability and the likelihood of the data given the hypothesis.</li>
            </ul>

\[P(H|D) = \frac{P(D|H) \cdot P(H)}{P(D)}\]

            <ul>
              <li>\(P(H|D)\) is the posterior probability, 
 \(( P(D|H) )\) is the likelihood, 
 \(( P(H) )\) is the prior probability, and 
 \(( P(D) )\) is the probability of the data.</li>
            </ul>
          </li>
          <li><strong>Incorporation of Prior Knowledge:</strong>
            <ul>
              <li>Bayesian analysis allows the incorporation of prior knowledge into the analysis, enabling the adjustment of beliefs based on both prior information and new data.</li>
            </ul>
          </li>
        </ol>

        <p>### Frequentism (Ronald A. Fisher):</p>

        <ol>
          <li><strong>Probability as Frequency:</strong>
            <ul>
              <li>Frequentist statistics, on the other hand, views probability as a long-run frequency or limit of relative frequencies. Probability is associated with the repeated occurrence of events in a hypothetical infinite sequence of trials.</li>
            </ul>
          </li>
          <li><strong>No Prior Probability:</strong>
            <ul>
              <li>Unlike Bayesianism, frequentist statistics does not involve prior probabilities. It relies solely on observed data and the likelihood of obtaining that data under different hypotheses.</li>
            </ul>
          </li>
          <li><strong>Hypothesis Testing and Confidence Intervals:</strong>
            <ul>
              <li>Frequentist methods often focus on hypothesis testing and confidence intervals. Hypothesis testing involves assessing the evidence against a null hypothesis, and confidence intervals provide a range of plausible values for an unknown parameter.</li>
            </ul>
          </li>
          <li><strong>Objective and Reproducible:</strong>
            <ul>
              <li>Frequentist methods are considered more objective and reproducible, as they do not rely on subjective prior beliefs.</li>
            </ul>
          </li>
        </ol>

        <p>### Comparison:</p>

        <ul>
          <li><strong>Subjectivity:</strong>
            <ul>
              <li>Bayesianism is often criticized for being subjective due to the reliance on prior beliefs. Frequentism is considered more objective since it doesn’t involve prior probabilities.</li>
            </ul>
          </li>
          <li><strong>Flexibility vs. Objectivity:</strong>
            <ul>
              <li>Bayesian methods provide a flexible framework for incorporating prior information, but this flexibility is seen by some as a weakness. Frequentism, being more rigid, is valued for its objectivity and reproducibility.</li>
            </ul>
          </li>
          <li><strong>Interpretation of Probability:</strong>
            <ul>
              <li>Bayesianism interprets probability as a measure of belief, while frequentism interprets it as a long-term frequency.</li>
            </ul>
          </li>
        </ul>

        <p>Both Bayesian and frequentist approaches have their strengths and weaknesses, and the choice between them often depends on the nature of the problem, available data, and the philosophical stance of the statistician or researcher. In practice, researchers sometimes use a combination of both approaches, known as Bayesian-frequentist synthesis.</p>

        <ul>
          <li>parameters
            <ul>
              <li>updated (represented probabilistically) vs. fixed</li>
              <li>with prior vs. no prior</li>
            </ul>
          </li>
          <li>data
            <ul>
              <li>fixed vs. random samples</li>
            </ul>
          </li>
          <li>example
            <ul>
              <li>diagnosis vs. dice</li>
            </ul>
          </li>
          <li>focused on
            <ul>
              <li>posterior vs likelihood</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>

    <p><img src="../../../assets/ML/ProbAndStats/Untitled 5.png" /></p>

    <ul>
      <li>conjugate distributions
        <ul>
          <li><a href="https://en.wikipedia.org/wiki/Conjugate_prior">https://en.wikipedia.org/wiki/Conjugate_prior</a></li>
          <li>Prior and posterior distribution is in the same family</li>
        </ul>
      </li>
      <li>Central Limit Theorem</li>
      <li>Cramer’s theorem
        <ul>
          <li><a href="https://carstart.tistory.com/160">https://carstart.tistory.com/160</a></li>
        </ul>
      </li>
      <li>multivariate Gaussian
        <ul>
          <li><a href="https://dhpark1212.tistory.com/entry/%EB%8B%A4%EB%B3%80%EB%9F%89-%EA%B0%80%EC%9A%B0%EC%8B%9C%EC%95%88-%EB%B6%84%ED%8F%ACMultivariate-Gaussian-Distribution">https://dhpark1212.tistory.com/entry/다변량-가우시안-분포Multivariate-Gaussian-Distribution</a></li>
        </ul>
      </li>
      <li>transformed Gaussian</li>
      <li>whitening transformation
        <ul>
          <li>it transforms a random variable vector with covariance into a new variable vector whose covariance is the identity matrix</li>
        </ul>
      </li>
      <li>transformed densities</li>
      <li>limitations of Gaussian distribution
        <ul>
          <li>there are too many parameters in general</li>
          <li>it is intrinsically unimodal</li>
        </ul>
      </li>
      <li>stationary process</li>
      <li>basic distributions: uniform</li>
      <li>basic distributions: Bernoulli
        <ul>
          <li>Discrete probability distribution</li>
        </ul>
      </li>
      <li>basic distributions: binomial
        <ul>
          <li>the probability of observing occurrences of in a set of samples from a Bernoulli distribution</li>
        </ul>
      </li>
      <li>basic distributions: categorical and multinomial</li>
    </ul>
  </li>
</ul>]]></content><author><name>JSC</name><email>newwin0198@handong.ac.kr</email></author><category term="ML" /><category term="ML" /><summary type="html"><![CDATA[Prob and Stats are the fundamentals. Prof. Henry Choi]]></summary></entry><entry><title type="html">2023-HGU-ML Lecture 1. Introduction to AI and ML</title><link href="http://localhost:4000/recent/Introduction_to_AI_ML/" rel="alternate" type="text/html" title="2023-HGU-ML Lecture 1. Introduction to AI and ML" /><published>2023-12-17T00:00:00+09:00</published><updated>2023-12-17T00:00:00+09:00</updated><id>http://localhost:4000/recent/Introduction_to_AI_ML</id><content type="html" xml:base="http://localhost:4000/recent/Introduction_to_AI_ML/"><![CDATA[<h1 id="introduction-to-ai--ml">Introduction to AI &amp; ML</h1>

<p>Came from Henry Choi’s Lecture and ChatGPT’s explanation</p>

<ul>
  <li>Introduction to AI
    <ul>
      <li>Weak and Strong AI
        <ul>
          <li>strong AI: understanding Chinese</li>
          <li>weak AI: simulating the ability to understand Chinese</li>
        </ul>
      </li>
      <li>Applied AI and General AI</li>
      <li>Computationalism and Connectionism
        <ul>
          <li>Computationalism
            <ul>
              <li>thoughts are computation on symbols
                <ul>
                  <li>Symbolic, interpretable</li>
                  <li>e.g) Turing Machine</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>Connectionism
            <ul>
              <li>Information is represented in neurons and networks
                <ul>
                  <li>Low-level, black-box</li>
                  <li>Neural Networks</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Turing Test</li>
      <li>Implementation level and algorithmic level
        <ul>
          <li>Implmentation level: how the system is physically realized
            <ul>
              <li>AI and Human Intelligence are different</li>
            </ul>
          </li>
          <li>Algorithmic level: how the system does, what representation or process it uses
            <ul>
              <li>3 level for intelligent system
                <ul>
                  <li>Learning, Computational level(what the system does and why), Algorithmic level</li>
                </ul>
              </li>
              <li>Can achieve some intelligence on approximation</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Superintelligence
        <ul>
          <li>bootstrapping from “child machine”, brain emulation, biological cognition, brain-computer interface, networks, and organizations</li>
        </ul>
      </li>
      <li>Deep Neural Networks
        <ul>
          <li>Hebbian learning, perceptron, multilayer perceptron, deep neural networks</li>
        </ul>
      </li>
      <li>Practical AI risks
        <ul>
          <li>affected by viruses</li>
          <li>misused by people with bad intentions</li>
          <li>biased AI</li>
          <li>taking over roles</li>
          <li>unable to reject AI’s decision</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Introduction to ML
    <ul>
      <li>What is learning
        <ul>
          <li>a computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, <strong>if its performance at the tasks improves with the experiences</strong></li>
          <li>simple model (e.g., linear regression)</li>
          <li>for supervised learning
            <ul>
              <li>Learning</li>
              <li>Recognition</li>
            </ul>
          </li>
          <li>Complex models</li>
        </ul>
      </li>
      <li>Machine learning
        <ul>
          <li>Takes data and output  → Makes program</li>
          <li>source of knowledge is data</li>
        </ul>
      </li>
      <li>Workflow of machine learning
        <ul>
          <li>acquisition - data is gathered/collected from various sources</li>
          <li>preparation - data is cleaned, preprocessed, and eventually becomes a dataset</li>
          <li>analysis - data is evaluated to run and customize reports</li>
          <li>modeling - data is patternized and generalized as models</li>
          <li>visualization</li>
          <li>deployment and maintenance</li>
        </ul>
      </li>
      <li>Components of ML
        <ul>
          <li>data: features, label, format</li>
          <li>models: SVM, NN, K-means</li>
          <li>objectives: cross-entropy, RMSE, likelihood</li>
          <li>optimization - gradient descent, Newton, linear programming, convex optimization</li>
        </ul>
      </li>
      <li>Data
        <ul>
          <li>structured/unstructured</li>
        </ul>
      </li>
      <li>Categories
        <ul>
          <li>unsupervised learning
            <ul>
              <li>e.g., clustering, dimension reduction</li>
            </ul>
          </li>
          <li>supervised learning
            <ul>
              <li>e.g., speech/face recognition</li>
            </ul>
          </li>
          <li>semi-supervised learning
            <ul>
              <li>e.g., cancer detection</li>
            </ul>
          </li>
          <li>reinforcement learning
            <ul>
              <li>e.g., AlphaGo, self-driving car</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Discriminative model and Generative model
        <ul>
          <li>
            <table>
              <tbody>
                <tr>
                  <td>Discriminative models: p(t</td>
                  <td>x)</td>
                </tr>
              </tbody>
            </table>
            <ul>
              <li>only for supervising</li>
            </ul>
          </li>
          <li>
            <table>
              <tbody>
                <tr>
                  <td>Generative model: p(t, x) or p(x</td>
                  <td>t)</td>
                </tr>
              </tbody>
            </table>
            <ul>
              <li>applicable to unlabeled data</li>
              <li>focusing on modeling each class’ distribution</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Pattern Recognition
        <ul>
          <li>measuring → preprocessing → dimensionality reduction → prediction → model selection
  -</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>JSC</name><email>newwin0198@handong.ac.kr</email></author><category term="ML" /><category term="ML" /><summary type="html"><![CDATA[AI and ML Intro. Prof. Henry Choi]]></summary></entry><entry><title type="html">Clean Code Chapter 3</title><link href="http://localhost:4000/recent/Clean_Code_Ch3/" rel="alternate" type="text/html" title="Clean Code Chapter 3" /><published>2023-07-16T00:00:00+09:00</published><updated>2023-07-16T00:00:00+09:00</updated><id>http://localhost:4000/recent/Clean_Code_Ch3</id><content type="html" xml:base="http://localhost:4000/recent/Clean_Code_Ch3/"><![CDATA[<p>How can we make a function communicate its intent?</p>

<p>What attributes can we give that will allow a reader to intuit the kind of program they live inside?</p>

<p><strong><em>The function should be very small</em></strong></p>

<ul>
  <li>In 1999, every function was just two, three, or four lines long
    <ul>
      <li>Each was transparent and led to the next in a compelling order</li>
    </ul>
  </li>
  <li>Block and Indenting
    <ul>
      <li>block within if, else, while and so on should be one line long
        <ul>
          <li>The line must be a function call</li>
          <li>The function should not be large enough to hold the nested structure</li>
          <li>The indent level of a function should not be greater than one or two</li>
        </ul>
      </li>
    </ul>

    <p><strong><em>Function Should Do One Thing</em></strong></p>
  </li>
  <li>What is one thing?
    <ul>
      <li>If a function does only that is on one level below the stated name of the function, then the function is doing one thing</li>
      <li>Decomposing a larger concept</li>
      <li>Simply restates the code without changing the level of abstraction</li>
      <li>if you can extract another function from it with a name that is not merely a restatement of its implementation</li>
      <li>A function that does one thing cannot be divided</li>
    </ul>
  </li>
  <li>One level of abstraction per function
    <ul>
      <li>Statements within the function are all at the same level of abstraction</li>
    </ul>
  </li>
  <li>Reading code from Top to Bottom: The Stepdown Rule
    <ul>
      <li>Top-down Narrative</li>
      <li>Reading program, descending one level of abstraction at a time as we read down the list of function</li>
      <li>Each function introduces the nest, and each function remains at a consistent level of abstraction</li>
    </ul>
  </li>
  <li>Avoid using switch statements
    <ul>
      <li>Each statement is buried in a low-level class and is never repeated</li>
      <li>Switch Statements can be tolerated if they appear only once</li>
      <li>Used to create polymorphic objects; hidden behind an inheritance relationship</li>
    </ul>
  </li>
  <li>Simple Responsibility Principle
    <ul>
      <li><a href="https://en.wikipedia.org/wiki/Single-responsibility_principle">https://en.wikipedia.org/wiki/Single-responsibility_principle</a></li>
    </ul>
  </li>
  <li>Open Closed Principle
    <ul>
      <li><a href="https://en.wikipedia.org/wiki/Open%E2%80%93closed_principle">https://en.wikipedia.org/wiki/Open–closed_principle</a></li>
    </ul>
  </li>
  <li>Use Descriptive Names
    <ul>
      <li>Choose descriptive names to clarify the design of the module in mind</li>
      <li>Don’t be afraid to make a name long</li>
      <li>Be consistent in names</li>
    </ul>
  </li>
</ul>

<p><strong><em>The argument is at a different level of abstraction</em></strong></p>

<ul>
  <li>Function Arguments
    <ul>
      <li>The ideal number argument for a function is zero(niladic)</li>
      <li>An argument forces the user to know the details</li>
    </ul>
  </li>
  <li>Arguments are hard at the testing point of a view
    <ul>
      <li>Need to test all combinations of arguments</li>
      <li>Output arguments are harder to understand than input arguments</li>
      <li>Don’t expect information to be going out through arguments</li>
    </ul>
  </li>
  <li>Common Monadic(one argument) function
    <ul>
      <li>Usage of arguments
        <ul>
          <li>Asking questions about arguments</li>
          <li>Operating on arguments, transforming the arguments</li>
        </ul>
      </li>
      <li><em>Event: there are input arguments but no output arguments</em>
        <ul>
          <li>The intention of calling the event is to use the argument the alter the state of the system</li>
          <li>Usage should be very clear that reader must notice this function is the event</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Flag Arguments
    <ul>
      <li><em>It is Ugly</em></li>
      <li>Split function instead of flag</li>
    </ul>
  </li>
</ul>

<p>Dyadic Functions</p>

<ul>
  <li>Should be avoided, but there is a situation in that dyadic function is necessary</li>
  <li>Take advantage of what mechanisms may be available to convert them into single-argument functions</li>
</ul>

<p>Triads Functions</p>

<ul>
  <li>Very careful to use it</li>
</ul>

<p>Argument Objects</p>

<ul>
  <li>When a group of variables is passed together, it is likely part of a concept that deserves a name of its own</li>
  <li>Augment list
    <ul>
      <li>Functions that take variable arguments can be monads, dyads, or even triads, but no more than that</li>
    </ul>
  </li>
</ul>

<p>Verb and Keywords</p>

<ul>
  <li>Monad case
    <ul>
      <li>the function and argument should form a very nice verb/noun pair</li>
    </ul>
  </li>
  <li>Using keywords to form a function name</li>
  <li>Function name and ordering of argument</li>
  <li>No side Effect
    <ul>
      <li>No temporal coupling</li>
      <li>State exactly what the function does and split the function if it is possible</li>
    </ul>
  </li>
  <li>Output arguments
    <ul>
      <li>Anything that forces you to check the function signature is equivalent to a double-take</li>
      <li><em>this</em> keyword is intended to act as an output argument</li>
      <li>Output argument should be avoided</li>
    </ul>
  </li>
</ul>

<p><strong><em>Do something or answer something, but not both</em></strong></p>

<p>Command Query Separation</p>

<ul>
  <li>Separate the command from the query so that ambiguity cannot occur</li>
</ul>

<p>Prefer Exception to Return Error Codes</p>

<ul>
  <li>Processing code can be separated from the happy path code and can be simplified</li>
  <li>Extract Try/Catch Block
    <ul>
      <li>Specific function for error handling</li>
      <li>It makes help to clarify the function
        <ul>
          <li>Nice Separation</li>
        </ul>
      </li>
      <li>Error Handling is one thing; function should only be one thing</li>
    </ul>
  </li>
  <li>New exceptions are derivatives of the exception class</li>
</ul>

<p>Don’t Repeat Yourself</p>

<ul>
  <li>Duplication may be the root of all evil in software</li>
  <li>Since the invention of the subroutine, innovations have been an ongoing attempt to eliminate duplication from our source code</li>
</ul>

<p>Structured Programming</p>

<ul>
  <li>Only be one <em>return</em> statement</li>
  <li>No <em>break</em> or <em>continue</em> statement</li>
  <li>Never <em>goto</em> statement</li>
  <li>These rules are beneficial for large functions, not small functions</li>
</ul>

<p>How to write a function with these rules?</p>

<ul>
  <li>Write code</li>
  <li>Message and refine the code, shrink and reorder, and keeping the test passing</li>
</ul>

<p>Conclusion</p>

<ul>
  <li>Functions are the verb</li>
  <li>Classes are the nouns</li>
  <li>The art of programming is the art of language design</li>
  <li>A function will be short, well-named, nicely organized</li>
  <li><strong><em>Tell the story of the system</em></strong>
    <ul>
      <li>need to fit cleanly together in precise language</li>
    </ul>
  </li>
</ul>]]></content><author><name>JSC</name><email>newwin0198@handong.ac.kr</email></author><category term="Books" /><category term="Blog" /><summary type="html"><![CDATA[Using Function Properly]]></summary></entry><entry><title type="html">And Then There Were None 그리고 아무도 없었다</title><link href="http://localhost:4000/recent/And_Then_There_Were_None/" rel="alternate" type="text/html" title="And Then There Were None 그리고 아무도 없었다" /><published>2023-07-16T00:00:00+09:00</published><updated>2023-07-16T00:00:00+09:00</updated><id>http://localhost:4000/recent/And_Then_There_Were_None</id><content type="html" xml:base="http://localhost:4000/recent/And_Then_There_Were_None/"><![CDATA[<p>추리 소설을 추천받았다. 아가사 크리스티의 <strong><em>그리고 아무도 없었다</em></strong> 였다. <br /> 
소설을 그렇게 좋아하는 편은 아니라 이름만 들어본 작가였지만 오랜만에 소설을 읽어보자는 마음으로 책을 집어들었다.</p>

<p>와우. 정말 재밌게 읽었다. 그 자리에 앉아서 내리 2시간을 읽었다. <del>물론 소설이 짧기도 하다</del> <br />
알고 보니 내 주위는 다들 읽었더라… 이 재밌는 걸 자신들만 알고 있었다니…</p>

<p>물론 선입견빼면 시체인 내가 추리 소설을 추천 받았더라도 내가 내키지 않았으면 안 읽었을 것 같긴 하다.
뭐든지 한 번 정도 시도해보고 판단하는게 맞는 것 같다는 생각이 근래에 많이 든다.</p>

<p>그 정도로 책이 재밌었다. 다음엔 <strong><em>오리엔트 특급 살인</em></strong> 읽어볼까 싶다!</p>]]></content><author><name>JSC</name><email>newwin0198@handong.ac.kr</email></author><category term="Books" /><category term="Blog" /><summary type="html"><![CDATA[재밌다...]]></summary></entry><entry><title type="html">Clean Code Chapter 2</title><link href="http://localhost:4000/recent/Clean_Code_Ch2/" rel="alternate" type="text/html" title="Clean Code Chapter 2" /><published>2023-07-13T00:00:00+09:00</published><updated>2023-07-13T00:00:00+09:00</updated><id>http://localhost:4000/recent/Clean_Code_Ch2</id><content type="html" xml:base="http://localhost:4000/recent/Clean_Code_Ch2/"><![CDATA[<p><strong>Using the appropriate Name will pay off in the short term and continue to pay in the long run</strong></p>

<ul>
  <li>Using Intention-Revealing Names
    <ul>
      <li>Choosing good names takes time but saves more than it takes</li>
      <li>Choosing a name that specifies what is being measured and the unit of that measurement</li>
      <li>Things need to think
        <ul>
          <li>What kind are in</li>
          <li>What is returned</li>
          <li>zeroth subscript and value</li>
        </ul>
      </li>
      <li>Do not use <strong>O <em>and l</em></strong></li>
    </ul>
  </li>
  <li>Avoid Disinformation
    <ul>
      <li>Avoiding words whose entrenched meanings vary from intention</li>
      <li>Spelling similar concepts similarly
        <ul>
          <li>No inconsistent spelling</li>
        </ul>
      </li>
      <li><del>Using different font??</del></li>
    </ul>
  </li>
  <li>Meaningful Distinction
    <ul>
      <li><strong>a</strong> for a local variable, <strong>the</strong> for all function arguments</li>
      <li>Less redundant words</li>
      <li>Distinguishing names in a way that readers know what the difference offer</li>
    </ul>
  </li>
  <li>Using Pronounceable Names
    <ul>
      <li>Essential for communication</li>
    </ul>
  </li>
  <li>Using Searchable Names
    <ul>
      <li>Single-letter names can ONLY be used as local variables inside short methods</li>
      <li>The length of a name should correspond to the size of its scope</li>
    </ul>
  </li>
  <li>Avoiding Encoding</li>
  <li>Hungarian Notation
    <ul>
      <li>The compiler knows the type nowadays</li>
    </ul>
  </li>
  <li>Member Prefixes
    <ul>
      <li>unseen clutter and a marker of older code…</li>
    </ul>
  </li>
  <li>Interface and Implementation
    <ul>
      <li>Why would I let user knows I’m handling them with an interface?</li>
      <li>Marking at the implementation</li>
    </ul>
  </li>
  <li>Avoid Mental Mapping
    <ul>
      <li>In most contexts, a single letter for a loop is a poor choice (such as i, j, k)</li>
      <li><em>Clarity is king</em></li>
    </ul>
  </li>
  <li>Class Names
    <ul>
      <li>should have noun or noun phrase names</li>
    </ul>
  </li>
  <li>Method Names
    <ul>
      <li>should have verb or verb phase names</li>
      <li>When constructors are overloaded, use the static method with names that describe the argument
        <ul>
          <li>Make corresponding constructors private</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Don’t be cute
    <ul>
      <li>No too “user-specific name”</li>
    </ul>
  </li>
  <li>Pick One Word per Concept
    <ul>
      <li>Pick one word for one abstract concept</li>
      <li>The name will express two different types of object</li>
    </ul>
  </li>
  <li>Don’t Pun
    <ul>
      <li>Avoiding using the same word for two purposes</li>
      <li>Even if it has a similar concept, it must have a different name if it is semantically different</li>
      <li><strong><em>It is the author’s responsibility</em></strong></li>
    </ul>
  </li>
  <li>Use Solution Domain Names
    <ul>
      <li>People who read your code will be programmers
        <ul>
          <li>CS terms, algorithm names, pattern names, math terms, etc.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Use Problem Domain Names
    <ul>
      <li>Separating solution and problem domain concept</li>
    </ul>
  </li>
  <li>Add Meaningful Context
    <ul>
      <li>Place the name in context for the reader
        <ul>
          <li>enclosing classes, functions, or namespaces</li>
        </ul>
      </li>
      <li>break it into smaller functions if it is necessary</li>
    </ul>
  </li>
  <li>But Don’t Add Gratuitous Context
    <ul>
      <li>Shorter names are generally better
        <ul>
          <li>As long as they are clear</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>JSC</name><email>newwin0198@handong.ac.kr</email></author><category term="Books" /><category term="Blog" /><summary type="html"><![CDATA[How to name clearly]]></summary></entry><entry><title type="html">Can We Automatically Fix Bugs by Learning Edit Operations?</title><link href="http://localhost:4000/recent/Can-We-Automatically-Fix-Bugs-by-Learning-Edit-Operations/" rel="alternate" type="text/html" title="Can We Automatically Fix Bugs by Learning Edit Operations?" /><published>2023-07-13T00:00:00+09:00</published><updated>2023-07-13T00:00:00+09:00</updated><id>http://localhost:4000/recent/Can%20We%20Automatically%20Fix%20Bugs%20by%20Learning%20Edit%20Operations</id><content type="html" xml:base="http://localhost:4000/recent/Can-We-Automatically-Fix-Bugs-by-Learning-Edit-Operations/"><![CDATA[<h2 id="can-we-automatically-fix-bugs-by-learning-edit-operations">“Can We Automatically Fix Bugs by Learning Edit Operations?”</h2>

<h2 id="paper"><a href="https://www.cs.wm.edu/~denys/pubs/SANER-RENE-BugFixing.pdf">Paper</a></h2>

<h3 id="summary">Summary:</h3>

<ul>
  <li>
    <p>Implementing Hephaestus, a novel method to improve the accuracy of APR through learning to apply edit operations. Leverages neural machine translation and attempts to produce the edit operations needed.</p>

    <p>Learning edit operations does not offer an advantage over the standard approach of translating directly from buggy code to fixed code. However, interestingly, Hephaestus exhibited lower translation accuracy than the baseline, able to perform successful bug repair.</p>
  </li>
</ul>

<h3 id="points">Points:</h3>

<ol>
  <li>Introduction
    <ol>
      <li>The naive approach attempts some sort of comparison algorithm that identifies the type of bug and replaces it with a prescribed bugs
        <ol>
          <li>Time Consuming</li>
        </ol>
      </li>
      <li>Learning approach using neural machine translation</li>
      <li>Directly applying the NMT approach to source code is inefficient
        <ol>
          <li>Many bugs fixes involve changes to a few sentences
            <ol>
              <li>Results in suboptimal performances</li>
            </ol>
          </li>
          <li>Attempt to mitigate the inefficiency by predicting the specific statement on AST</li>
          <li>Attempting on individual tokens would be more optimal</li>
        </ol>
      </li>
      <li>Hephaestus leverages NMT to predict edit operation, derived from Levenshtein Distance Algorithm
        <ol>
          <li>Working at the token level of source code</li>
          <li>Work on any language without language-specific parsers</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>Related Works
    <ol>
      <li>Tufano et al.
        <ol>
          <li>Repairing code through identification of bug-fix patterns in large software repositories</li>
          <li>Usage of Deep Learning Approach regarding “meaningful” change</li>
        </ol>
      </li>
      <li>Chen et al.
        <ol>
          <li>Focus on single-line bug</li>
        </ol>
      </li>
      <li>Jiang et al.
        <ol>
          <li>The correct fix for a given bug does not exist within the model’s output space and the model’s lack of awareness of syntax</li>
          <li>Pre-train model on the programming language in question</li>
        </ol>
      </li>
      <li>Yuan and Banzhaf
        <ol>
          <li>grouping fine-granularity edits into larger statement-level edits</li>
        </ol>
      </li>
      <li>Mousavi et al.
        <ol>
          <li>Overfitting and Disparity between predicted bug and fix operation and would mimic a human software developer</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>Background
    <ol>
      <li>Fixing buggy code to fixed code using traditional language translation matter of the buggy to fix a variety of language</li>
      <li>Traditional translation replaces the majority of the input sequence which is natural language.
        <ol>
          <li>Fix in code might be minimal</li>
          <li>repair translation should not have the same meaning as the input</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>Approach
    <ol>
      <li>Levenshtein Edit Operation
        <ol>
          <li>the bug is input sequence, the NMT model attempts to produce edit operations</li>
          <li>Basic Operations
            <ol>
              <li>Insertion</li>
              <li>Deletion</li>
              <li>Replacement</li>
            </ol>
          </li>
        </ol>
      </li>
      <li>Compound Edit Operations
        <ol>
          <li>Group of one or more edit operations; sequence of operations</li>
          <li>Condensing: A grouping process to compound operations</li>
        </ol>
      </li>
      <li>Dataset Construction
        <ol>
          <li>Control Dataset: baseline, not involved with edit operations</li>
          <li>Machine String: In order to include edit operations, transforming edit operations
            <ol>
              <li>Typed</li>
              <li>General</li>
            </ol>
          </li>
          <li>We make the distinction between typed and general form to determine if the form of machine string used during training affects the Hephaestus models’ abilities to learn edit operations.</li>
        </ol>
      </li>
      <li>Experimental Dataset
        <ol>
          <li>translate the bug into its corresponding fix, showing Levenshetein edit distance between the bug and fix</li>
          <li>all basic compound operation sequences which transform the bug into the fix, strict is the minimal sequence of the strict compound operation sequences, and loose is the minimal sequence of the loose compound operation sequences</li>
        </ol>
      </li>
      <li>Model Construction
        <ol>
          <li>LSTM+General</li>
          <li>GRU+General</li>
          <li>LSTM+Typed</li>
        </ol>
      </li>
      <li>The CEC ensures that error signals fed forward into the LSTM layers and backpropagated to the LSTM layers are resistant to the effects of the vanishing gradient problem.</li>
    </ol>
  </li>
  <li>Experimental Design
    <ol>
      <li>Perfect Prediction Accuracy</li>
      <li>Failed Prediction Rate</li>
      <li>Edit Distance Decrease</li>
      <li>Training Accuracy</li>
    </ol>
  </li>
  <li>Result
    <ol>
      <li>PPA: The control model (baseline model) outperformed the rest, with no much difference</li>
      <li>FPR: The control model maintained 100% capability, the string can always be interpreted as a sequence of Java method tokens</li>
      <li>EDD: every model generates “bug fixes” that were further away from the fixed code than the original buggy code</li>
      <li>Training Accuracy: Every model exceeded 90%</li>
    </ol>
  </li>
  <li>RQ
    <ol>
      <li>RQ1: Is learning edit operations an effective approach to automatic bug repair?
        <ol>
          <li>learning edit operations does not offer advantages over the baseline approach. The experimental Hephaestus models must determine a sequence of edit operations, decode them, and apply them to the inputted buggy method in order to predict fixed source code</li>
        </ol>
      </li>
      <li>RQ2: What effect does each condensing strategy and machine string form have on the accuracy of bug repair?
        <ol>
          <li>The differences in PPA between the basic, strict, and loose models are negligible, but there are differences according to the training accuracy and average EDD values. Despite having significantly lower final training accuracy, the strict and loose models had slightly more positive EDD values than the basic models (a difference of about 0.96). Thus, it is evidenced that condensing edit operations into strict and loose forms are beneficial over not condensing them at all</li>
        </ol>
      </li>
      <li>RQ3: What is the effect of using an LSTM-based architecture versus a GRU-based architecture on the accuracy of bug repair?
        <ol>
          <li>the variation is not meaningful enough to consider as a key difference between the models.</li>
        </ol>
      </li>
      <li>Future Work
        <ol>
          <li>It was determined that most failed predictions were caused by generated indices outside the valid range for a given string. What changes can be made to this model to restrict the prediction range?</li>
          <li>does changing the abstraction method of the training dataset affect this metric?</li>
          <li>Other NLP Tools</li>
          <li>Extra software layers in addition to the methods presented in our study.</li>
        </ol>
      </li>
      <li>Conclusion
        <ol>
          <li>The introduction of these specific methods for training NMTbased systems to learn bug fixes did not provide a benefit to the task</li>
          <li>Edit operations are capable of performing automated bug repair to some degree</li>
        </ol>
      </li>
    </ol>
  </li>
</ol>

<h3 id="knowledge">Knowledge:</h3>

<ul>
  <li>NMT (Neural Machine Translation): <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">Neural machine translation - Wikipedia</a>uses an artificial neural work to predict the likelihood of a sequence of words</li>
  <li>Levenshtein Distance Algorithm: A string metric for measuring the difference between two sequences. This is likely due to the experimental models experiencing higher entropy than the control when making predictions.</li>
</ul>

<h3 id="terminology">Terminology:</h3>

<ul>
  <li>Condensing Strategies:
    <ul>
      <li>Basic Condensing: basic compound operation corresponds with exactly one change</li>
      <li>Loose Condensing: iff the application of its constituent operation is equivalent to the application of some singular op
        <ul>
          <li>Modify a contiguous section of tokens</li>
        </ul>
      </li>
      <li>Strict Condensing: iff it is loosely compatible and every operation is of the same flavor</li>
    </ul>
  </li>
  <li>Machine Strings:
    <ul>
      <li>Typed: f is one of ins, del, or rep, depending on if the flavor of the represented edit operation is insertion, deletion, or replacement, respectively</li>
      <li>General: general form machine strings do not explicitly store the flavor of their represented edit operations</li>
    </ul>
  </li>
</ul>

<h3 id="tool">Tool:</h3>

<p><a href="https://github.com/WM-SEMERU/hephaestus">GitHub - WM-SEMERU/hephaestus</a></p>

<h3 id="questions">Questions:</h3>]]></content><author><name>JSC</name><email>newwin0198@handong.ac.kr</email></author><category term="APR" /><category term="APR" /><summary type="html"><![CDATA[Generalizing the change is helpful...?]]></summary></entry><entry><title type="html">Clean Code Chapter 1</title><link href="http://localhost:4000/recent/Clean_Code_Ch1/" rel="alternate" type="text/html" title="Clean Code Chapter 1" /><published>2023-07-13T00:00:00+09:00</published><updated>2023-07-13T00:00:00+09:00</updated><id>http://localhost:4000/recent/Clean_Code_Ch1</id><content type="html" xml:base="http://localhost:4000/recent/Clean_Code_Ch1/"><![CDATA[<ul>
  <li>What is Clean Code?
    <ul>
      <li>Bjarne Stroustrup, Inventor of C++
        <ul>
          <li>Wasted Cycle are inelegant, not pleasing</li>
          <li>Error handling should be complete</li>
          <li>Focused; Each function, class, and module expose a single-minded attitude that remains entirely undistracted, and unpolluted, by the surrounding details</li>
        </ul>
      </li>
      <li>Grady Booch, author of Object Oriented Analysis and Design with Application
        <ul>
          <li>He took a readability perspective</li>
          <li>Well-written purpose</li>
          <li>clean code should clearly expose the tensions in the problem to be solved</li>
          <li>Should be matter-of-fact as opposed to speculative</li>
        </ul>
      </li>
      <li>Dave Thomas, founder of OTI
        <ul>
          <li>Make it easy for other people to enhance it</li>
          <li>Code without test is not clean at all</li>
          <li>values code that is small, code should be literate</li>
        </ul>
      </li>
      <li>Michael Feathers, author of Working Effectively with Legacy Code
        <ul>
          <li>Looks like it was written by someone who cares</li>
          <li>Someone has taken the time to keep it simple and orderly</li>
        </ul>
      </li>
      <li>Ron Jeffries, author of Extreme Programming Installed and Extreme Programming Adventures in C#
        <ul>
          <li>No duplications</li>
          <li>Express all the design ideas
            <ul>
              <li>High expressiveness</li>
            </ul>
          </li>
          <li>Minimize the number of entities
            <ul>
              <li>Find things in a collection</li>
              <li>Wrap particular implementation in a more abstract method</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Ward Cunningham, inventor of Wiki, inventor of Fit …
        <ul>
          <li>read turns out what you expected</li>
          <li>make it look like the language was made for the problem</li>
          <li>It is the programmer that makes the language appear simple</li>
        </ul>
      </li>
      <li>Author: <a href="https://www.google.com/search?q=Robert+C.+Martin&amp;stick=H4sIAAAAAAAAAONgVuLWz9U3MDSqMspOT3rEaMYt8PLHPWEp3UlrTl5jVOfiCs7IL3fNK8ksqRSS5GKDsvileLmQ9fEsYhUIyk9KLSpRcNZT8E0sKsnMAwD4hk_pWwAAAA&amp;sa=X&amp;ved=2ahUKEwjDuvb5qYSAAxWJAt4KHYeWALQQzIcDKAB6BAgiEAE">Robert Cecil Martin</a>
        <ul>
          <li>Discovering new techniques and founding their own schools</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>Many Recommendations in this book are controversial.</p>

<p>And we are <strong>AUTHORS</strong>.</p>

<ul>
  <li>Making it Easy to read makes it easier to write</li>
  <li>The code has to be kept clean over time</li>
</ul>

<p>This book cannot promise to make you a good programmer.</p>

<p><strong>“Practice, son. Practice!”</strong></p>]]></content><author><name>JSC</name><email>newwin0198@handong.ac.kr</email></author><category term="Books" /><category term="Blog" /><summary type="html"><![CDATA[What is Clean Code?]]></summary></entry><entry><title type="html">Automatic Patch Generation with Context-based Change Application</title><link href="http://localhost:4000/recent/ConFix/" rel="alternate" type="text/html" title="Automatic Patch Generation with Context-based Change Application" /><published>2023-07-10T00:00:00+09:00</published><updated>2023-07-10T00:00:00+09:00</updated><id>http://localhost:4000/recent/ConFix</id><content type="html" xml:base="http://localhost:4000/recent/ConFix/"><![CDATA[<h3 id="automatic-patch-generation-with-context-based-change-application">Automatic Patch Generation with Context-based Change Application</h3>

<h2 id="paper"><a href="https://raw.githubusercontent.com/wiki/thwak/ConFix/pre-print.pdf">Paper</a></h2>

<p>Jindae Kim, Sunghun Kim</p>

<h3 id="goal">Goal:</h3>

<ol>
  <li>I am collecting changes from human-written patches for new patch candidate generation.</li>
  <li>Automatic patch generation technique leveraging human-written patches with our context-based change application technique used by ConFix.</li>
</ol>

<h3 id="summary">Summary:</h3>

<ul>
  <li>An effective patch generation technique should have a large search space with a high probability that patches for bugs are included, and it also needs to locate such patches effectively. Confix collects abstract AST changes from human-written patches, providing resources for patch generation. Then, using only matching context only, Confix selects a necessary change for a possible fixed location. Also, Confix filters out undesirable locations to fix using the info that the location has not been changed in human-written patches.</li>
</ul>

<h3 id="points">Points:</h3>

<ol>
  <li>The problem with APR is search space size and navigation.
    <ol>
      <li>To solve the problem, mining changes from human-written patches has been trying, but it makes navigation for the patch more difficult since the changes are very sparse.</li>
      <li>Confix, use context-based change application technique to generate patch candidates.
        <ol>
          <li>AST context applies collected change to a possible fixed location only if their AST contents are matched</li>
          <li>ConFix compares AST contexts defined by parent, left, and right nodes of a change and a location and applies the change to a target location only if their contexts are matched.</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>Change Collection
    <ol>
      <li>Collecting changes generally applicable to other location</li>
      <li>We extract AST form since source code can be easily affected by specific user styles.</li>
      <li>Following some steps, dependent on how changes are collected</li>
    </ol>
  </li>
  <li>Change Extraction
    <ol>
      <li>Each hunk is extracted and converted to an individual AST subtree change.</li>
      <li>Collect separate individual changes rather than the whole patch to use repetitiveness of changes.</li>
      <li>To extract changes, use the differencing tool, generates AST subtree edit operation, combine node edit operation into trees
        <ol>
          <li>AST node type and node value indicate a specific identifier, literals, or operators; the edit operation preserves the AST structure of the inserted code fragment</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>Change conversion
    <ol>
      <li>Extracted edit operations need to be independent and applicable form.
        <ol>
          <li>The type needs to be changed as a single change
            <ol>
              <li>divide operation more</li>
              <li>extends operation</li>
              <li>remove any operation if a changed AST is a subtree of another operation’s changed AST</li>
              <li>Find all inserted-delete pairs applied to the same location and combine them.</li>
            </ol>
          </li>
        </ol>
      </li>
      <li>Need to normalize the user-defined identifiers.
        <ol>
          <li>By normalizing user-defined names, we can increase the reusability of collected changes since we can apply a change regardless of the availability of user-defined names.</li>
          <li>The technique normalizes user-defined names with two principles: consistency and order-preserving</li>
          <li>By normalizing user-defined names, we can increase the reusability of collected changes since we can apply a change regardless of the availability of user-defined names.</li>
          <li>At the time of change concretization, the collected information works as requirements for each abstract name which ConFix should consider when it assigns a concrete name for the abstract name.</li>
        </ol>
      </li>
      <li>Change Context Identification
        <ol>
          <li>After individual changes are obtained from source code patches, the next step is identifying the AST contexts of the changes.</li>
          <li>By identifying the context of the target location, we can avoid meaningless modifications and more frequent changes.</li>
          <li>To define AST context, use nearby nodes of a changed AST subtree
            <ol>
              <li>The parent node represents the location where a changed code fragment belongs.</li>
              <li>The left and Right nodes indicate code fragments before and after the changed code fragment.</li>
              <li>Using fingerprinting technique</li>
              <li>Dyck Word Hash
                <ol>
                  <li>represents the parent-child relationship of nodes</li>
                  <li>Compare two nodes’ hash values to check whether they are the roots of two type-isomorphic AST subtrees.</li>
                  <li>For fast comparison</li>
                </ol>
              </li>
            </ol>
          </li>
          <li>PLRTH and PLRT
            <ol>
              <li>left node indicates code before the changed code frame; the right node indicates code after the changed code fragment</li>
              <li>PTLRH context, we also examine whether the change and the location have the same code fragment before and after them, while PLRT context only checks which kind of code fragments exist before and after the change and the location.</li>
              <li>PTLRH context constrains the search area too much; we can expand the search area with PLRT context.</li>
              <li>Exception</li>
              <li>Block node does not mean too much; use its parent node as a type.</li>
              <li>Move operations primarily consider the old location’s context, but store the new location’s context to use it as an additional constraint of move changes.</li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>
  </li>
  <li>Change Application
    <ol>
      <li>Target Location Context Identification
        <ol>
          <li>The target location is also an AST node to which a change will be applied</li>
          <li>To identify the location context is compatible with change contexts, using PLR nodes we can identify location contexts for changes
            <ol>
              <li>Except for insert, another operation has an old AST subtree so it is applicable</li>
              <li>The inserted context is identified from New AST which is inserted. We need a different type, based on the assumption that a subtree is inserted near a location. Insert Before and Insert After contexts are the contexts for cases in which a new AST subtree is inserted before and after node N. Node C indicates the actual location where a subtree will be inserted</li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>
  </li>
  <li>Change Selection
    <ol>
      <li>retrieve changes with the same context from a changing pool</li>
      <li>Categorized by their context, only store unique changes with their frequency</li>
      <li>choose one of the selections and mimic human-written changes by selecting any of the changes on the list
        <ol>
          <li>The random selection, strategy is not decided specifically</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>Change Concretiziation
    <ol>
      <li>Replacing all normalized identifiers with concrete name
        <ol>
          <li>Do not fully specify the strategy yet how ConFix concretizes a change since various strategies can be used</li>
          <li>Since the type of normalized variables and signature of normalized methods are also stored during change collection, we can consider a strategy that assigns an existing variable name to a normalized variable (if the type is compatible)</li>
          <li>Since change is concretized, apply it to the target location</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>Patch generation
    <ol>
      <li>Generate-and-validate process</li>
      <li>Starts with the PTLRH pool to narrow down the search area for patch generation, switch to PLRT if the patch generation failed at the above level</li>
      <li>Identify target location, retrieves changes having the same context as the selected location, apply it to the target location</li>
      <li>Lastly, tries different name assignments by predefined max trials
        <ol>
          <li>To prevent spoiled validation due to wrong name assignment</li>
        </ol>
      </li>
      <li>Pass -&gt; termination; Fail -&gt; continues with another change</li>
      <li>When it reaches the max candidate, moves to the next change pool</li>
    </ol>
  </li>
  <li>Target Location Identification
    <ol>
      <li>Confix leverages both fault localization and change context to identify target locations from given buggy code
        <ol>
          <li>Identifies all AST nodes which belong to the statement as potential target locations</li>
          <li>filters out all target locations having context not included in the current change pool</li>
          <li>Identify fail and pass a group
            <ol>
              <li>The first pick from the fail-only group, if there is no location, starts from the pass-fail group</li>
              <li>Failing test cases have a much higher priority</li>
            </ol>
          </li>
          <li>Prior target location has if, method invocation, infix expression, or return statement since it influences the whole execution</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>Change Selection
    <ol>
      <li>ConFix selects one of the retrieved changes for the current target location and chooses the most frequent change</li>
      <li>Even if it is in the same context, change might not be applicable
        <ol>
          <li>In case of replacement operation, one more location should be selected</li>
          <li>Therefore, ConFix randomly selects one of the target locations with a matching context
            <ol>
              <li>If there is not, discard the change</li>
            </ol>
          </li>
        </ol>
      </li>
      <li>Code might not be compilable, so ConFix goes through the verification steps
        <ol>
          <li>Examine all change-location pairs since a change that was applicable in another location might not be applicable in other locations</li>
          <li>Considering all pairs and compiling is beneficial rather than since it is not expensive to recompile source code with a small modification</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>Change Concretization
    <ol>
      <li>Concretizing selected change for the target location</li>
      <li>ConFix collects concrete names of variables, types, and methods from the given buggy code and decides which names are within the scope</li>
      <li>To find a concrete method for a normalized method, ConFix identifies compatible methods from collected methods available at a target location</li>
      <li>Abstract signature
        <ol>
          <li>method signature with normalized types</li>
          <li>The purpose of this is to find a concrete method with the same abstract signature, then it will not cause a compile error</li>
        </ol>
      </li>
      <li>Assignable type
        <ol>
          <li>Consider both type compatibility and the number of variables of the types
            <ol>
              <li>Normalized types are considered wildcard characters, which means that they can be assignable to either normalized types or JSL types.</li>
              <li>ConFix only considers a type as assignable to another type if there exist enough variables for assignment.</li>
            </ol>
          </li>
          <li>the compatibility of a normalized and a concrete method is defined by their abstract signatures and assignable types
            <ol>
              <li>The concrete method first considers local methods, then global methods with the assumption that local methods are more closely related to buggy code</li>
              <li>Once a concrete method for a normalized method is selected and assigned, ConFix update types are matched due to method assignment</li>
              <li>Randomly assigns one of the assignable concrete types for each normalized type</li>
              <li>Assigns concrete variables to normalized variables</li>
            </ol>
          </li>
          <li>There is one additional treatment for update type changes which updates an identifier with another identifier
            <ol>
              <li>Assume the update change is meaningful when the new name is similar to the old one, so calculate the Levenshtein distance between the identifier and concrete name</li>
            </ol>
          </li>
          <li>In case of no type-compatible assignment
            <ol>
              <li>Change concretization is considered failed and no patch candidate is generated</li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>
  </li>
  <li>Evaluation
    <ol>
      <li>Collected changes with PTLRH and PTLR contexts from Apache Commons Collections (collections), Derby (derby), Hadoop (hadoop), Ivy (ivy) and Lucene (lucene) projects.
        <ol>
          <li>we selected a completely different set of projects for change resources from the projects in Defects4j dataset.</li>
          <li>For each context, there are averages 1.91 and 16.25 changes in PTLRH and PLRT change pools respectively</li>
        </ol>
      </li>
      <li>We built change pools and obtained coverage information of all buggy codes before we applied ConFix to each bug.
        <ol>
          <li>Change pool and coverage information is given to ConFix</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>Results
    <ol>
      <li>Compilation of one Java file is much cheaper than test execution even if only a few failing test is executed. Therefore ConFix can find a patch within a reasonable time.</li>
      <li>Acceptable and Plausible</li>
      <li>We did not set a time budget, however, ConFix generated all the patches within two hours</li>
      <li>ConFix generated 71 patches
        <ol>
          <li>which are greater than other techniques - ssFix(60), Nopol(33), jGenProg(19), jKali(18), HDRepair(16) and ACS(7)</li>
          <li>generated 13 acceptable patches, which is significantly higher than valid patches generated by HDRepair(5), ACS(3), jGenProg(3), jKali(1), and Nopol(0). One exception is ssFix, which generated 20 valid patches, higher than ConFix.</li>
        </ol>
      </li>
      <li>ssFix
        <ol>
          <li>We verified ssFix-generated patches again and found that two valid patches (C1, M79) are not acceptable patches.</li>
        </ol>
      </li>
      <li>ConFix was able to find necessary changes and the right fix locations with its patch generation strategy</li>
      <li>Informative Patches
        <ol>
          <li>Although plausible patches are not acceptable when it is compared to human-written patches, they might be given as debugging hints for developers.</li>
          <li>we manually analyzed 58 plausible patches and checked whether these patches are informative.</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>Are PLRTH and PLRT really helpful?
    <ol>
      <li>With PTLRH contexts, ConFix explores a much narrower area in its candidate space
        <ol>
          <li>Consequently, it might also lose the chance that actual patches are included in the area</li>
        </ol>
      </li>
      <li>Among 71 generated patches, 81% of the patches (58/71) are generated by changes from PTLRH change pools. In terms of acceptable patches, using PLRT context only gives two more acceptable patches, and PTLRH change pool still works for 85% of the acceptable patches (11/13).</li>
      <li>ConFix generated two acceptable patches which take 15% of all acceptable patches under both types of context. These PLRT acceptable patches are all semantically equivalent to human-written patches, which means that they addressed issues in the same way as humans did. Therefore PLRT context can also provide practical constraints to mimic the developer’s changes and produce acceptable patches.</li>
    </ol>
  </li>
  <li>Threats to validity
    <ol>
      <li>Our evaluation results might be different if we used other collected changes from different human-written patches.</li>
      <li>There exists another issue that bugs from five projects might not be representative</li>
      <li>Manual assessment of patches could be another issue, since we do not have domain knowledge and the judgment about patches might be subjective and biased.</li>
    </ol>
  </li>
  <li>Related Works
    <ol>
      <li>The difference is that ConFix uses AST contexts to select one of the changes, while ssFix considers the syntactic relation of code fragments to given buggy code.</li>
      <li>Identifying AST contexts is less costly than deriving SMT constraints, but these contexts still provide syntactic information which also implies some of the program semantics, although it does not fully represent the program’s semantics like SMT constraints.</li>
      <li>ConFix differs from these previous techniques due to the point that it can automatically collect abstract individual changes on a large scale and it uses them to generate patch candidates, instead of generating patch candidates with several pre-defined modifications or mutation operations with limited resources of code fragments</li>
    </ol>
  </li>
  <li>Studies on Human-written patches
    <ol>
      <li>changes in bug fixes are repetitive, and smaller changes are even more repetitive</li>
      <li>There exist other studies on changes and source code’s uniqueness which imply the potential of techniques leveraging 17 existing code fragments or changes</li>
      <li>Empirical evaluation of ConFix and fixability analysis results imply that we can obtain necessary changes for new bug fixes from existing patches.</li>
    </ol>
  </li>
  <li>Change Collection and Application
    <ol>
      <li>we may consider using these code transfer techniques to develop new methods for patch candidate generation in ConFix</li>
      <li>it is possible to apply high-level ideas such as collecting abstract changes with their AST contexts regardless of adjustment</li>
    </ol>
  </li>
  <li>Conclusion
    <ol>
      <li>We may try to generate patches with multiple changes to improve partial patches up to acceptable patches or use more sophisticated concretization methods to effectively generate high-quality patches.</li>
    </ol>
  </li>
</ol>

<h3 id="knowledge">Knowledge:</h3>

<ul>
  <li>Context-based Change Application Technique: a technique to generate candidate patches</li>
  <li>ConFix can expand its search space by collecting more changes, while it can navigate through them effectively with the guidance of context</li>
</ul>

<h3 id="terminology">Terminology:</h3>

<ul>
  <li>Hunk: Single changes including deletion and addition, may have multiple in a single commit</li>
</ul>

<h3 id="questions">Questions:</h3>

<ul>
  <li>Why PLRTH and PLRT: The paper mentioned it is for reducing the search space. Is it the most efficient way to do it?</li>
  <li>I don’t understand the part about fault localization. Is it possible to localize the fault using AST context? Does it just localize fault according to the collected change?</li>
  <li>What if AST differencing tools work maliciously?</li>
</ul>]]></content><author><name>JSC</name><email>newwin0198@handong.ac.kr</email></author><category term="APR" /><category term="APR" /><summary type="html"><![CDATA[ConFix]]></summary></entry><entry><title type="html">Operating Systems: Three Easy Pieces Ch. 40</title><link href="http://localhost:4000/recent/Chapter-40/" rel="alternate" type="text/html" title="Operating Systems: Three Easy Pieces Ch. 40" /><published>2023-06-17T00:00:00+09:00</published><updated>2023-06-17T00:00:00+09:00</updated><id>http://localhost:4000/recent/Chapter-40</id><content type="html" xml:base="http://localhost:4000/recent/Chapter-40/"><![CDATA[<p>File System Implementation</p>

<ul>
  <li>Very Simple File System
    <ul>
      <li>The file system is pure software</li>
      <li>we will not be adding hardware features to make some aspect of the file system work better</li>
    </ul>
  </li>
  <li>The first is the data structure of the file system
    <ul>
      <li>data and metadata</li>
      <li>access method → open(), read(), write()</li>
    </ul>
  </li>
  <li>Overall Organization
    <ul>
      <li>Block
        <ul>
          <li>one block size, the common size of 4 KB</li>
          <li>size 4 KB</li>
        </ul>
      </li>
      <li>Data region</li>
      <li>Metadata
        <ul>
          <li>Inode</li>
          <li>Inode table</li>
        </ul>
      </li>
      <li>Allocation structures
        <ul>
          <li>Free list
            <ul>
              <li>points to the next free block</li>
            </ul>
          </li>
          <li>bitmap
            <ul>
              <li>Data region</li>
              <li>Inode</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Superblock
        <ul>
          <li>Contains information about this particular file system</li>
          <li>inode bitmap</li>
          <li>data bitmap</li>
          <li>Inodes</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>File Organization: The Inode
    <ul>
      <li>inode (short for index node)</li>
      <li>these nodes were originally arranged in an array, and the array indexed into when accessing a particular inode</li>
      <li>Each inode is implicitly referred to by a number (called the i-number), which we’ve earlier called the low-level name of the file</li>
      <li>20KB in size (five 4KB blocks) and thus consisting of 80 inodes (assuming each inode is 256 bytes)</li>
    </ul>
  </li>
  <li>metadata</li>
  <li>direct pointers</li>
  <li>The Multi-Level Index
    <ul>
      <li>Indirect Pointer
        <ul>
          <li>it points to a block that contains more pointers, each of which point to user data</li>
        </ul>
      </li>
      <li>If a file grows large enough, an indirect block is allocated</li>
      <li>the double indirect pointer</li>
      <li>triple is possible</li>
      <li>multi-level index approach to pointing to file blocks. Let’s examine an example with twelve direct pointers and a single and a double indirect block. Assuming a block size of 4 KB, and 4-byte pointers, this structure can accommodate a file of just over 4 GB in size</li>
      <li>Other file systems, including SGI XFS and Linux ext4, use extents instead of simple pointers</li>
      <li>Most files are small</li>
    </ul>
  </li>
  <li>Directory Organization
    <ul>
      <li>each entry has an inode number, record length (the total bytes for the name plus any left over space), string length (the actual length of the name), and finally the name of the entry</li>
      <li>Deleting the file → unlink()</li>
      <li>directory has an inode, somewhere in the inode table</li>
    </ul>
  </li>
  <li>free space management
    <ul>
      <li>pre-allocation policy</li>
      <li>the file system guarantees that a portion of the file will be contiguous on the disk</li>
    </ul>
  </li>
  <li>Access Path
    <ul>
      <li>The file system must traverse the pathname and thus locate the desired inode</li>
      <li>root directory</li>
      <li>i-number of file or directory</li>
      <li>read and lseek
        <ul>
          <li>read in the first block of the file, consulting the inode to find the location of such a block</li>
        </ul>
      </li>
      <li>Close
        <ul>
          <li>No disk I/Os take place</li>
        </ul>
      </li>
      <li>Writing to the file may also allocate a block
        <ul>
          <li>Overwritten</li>
        </ul>
      </li>
      <li>Traffic of reading and writing
        <ul>
          <li>one for inode bitmap, new inode, directory, and directory inode</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Caching and Buffering
    <ul>
      <li>fixed-size cache</li>
      <li>LRU</li>
      <li>static partioning
        <ul>
          <li>10% of the total memory</li>
        </ul>
      </li>
      <li>Dynamic Partioning
        <ul>
          <li>unified page cache</li>
        </ul>
      </li>
      <li>Write Buffering</li>
      <li>Batch</li>
      <li>Schedule</li>
      <li>Fsync()
        <ul>
          <li>direct I/O interfaces</li>
          <li>raw disk interface</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>JSC</name><email>newwin0198@handong.ac.kr</email></author><category term="OS" /><category term="OS" /><summary type="html"><![CDATA[OS Ch.40]]></summary></entry><entry><title type="html">Operating Systems: Three Easy Pieces Ch. 39</title><link href="http://localhost:4000/recent/Chapter-39/" rel="alternate" type="text/html" title="Operating Systems: Three Easy Pieces Ch. 39" /><published>2023-06-17T00:00:00+09:00</published><updated>2023-06-17T00:00:00+09:00</updated><id>http://localhost:4000/recent/Chapter-39</id><content type="html" xml:base="http://localhost:4000/recent/Chapter-39/"><![CDATA[<p>File and Directories</p>

<ul>
  <li>Persistence Storage
    <ul>
      <li>HDD</li>
      <li>SSD</li>
    </ul>
  </li>
  <li>Files and Directories
    <ul>
      <li>inode number</li>
      <li>directory
        <ul>
          <li>Directory tree (hierarchy)</li>
          <li>Root directory</li>
          <li>Subdirectory</li>
          <li>Separator</li>
        </ul>
      </li>
      <li>Type of the file</li>
    </ul>
  </li>
  <li>Creating file
    <ul>
      <li>open()</li>
      <li>file descriptor
        <ul>
          <li>Capability</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Reading and Writing files
    <ul>
      <li>Strace
        <ul>
          <li>trace the system calls made by a program</li>
          <li>Flags</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Not sequential reading and writing
    <ul>
      <li>random offset
        <ul>
          <li>searching specific word</li>
        </ul>
      </li>
      <li>File Offset</li>
      <li>Open file table
        <ul>
          <li>OS can use this to determine whether the opened file is readable or writable (or both), which underlying file it refers to (as pointed to by the struct inode pointer ip), and the current offset (off).</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Shared File Table Entities
    <ul>
      <li>Fork and dup</li>
      <li>reference count</li>
      <li>The dup() call allows a process to create a new file descriptor that refers to the same underlying open file as an existing descriptor</li>
    </ul>
  </li>
  <li>Fsync()
    <ul>
      <li>(DBMS), development of a correct recovery protocol requires the ability to force writes to disk from time to time</li>
    </ul>
  </li>
  <li>Renaming files
    <ul>
      <li>rename() call is atomic</li>
    </ul>
  </li>
  <li>Information about files
    <ul>
      <li>metadata
        <ul>
          <li>stat() or fstat() system calls</li>
        </ul>
      </li>
      <li>inode</li>
    </ul>

    <p><img src="../../../assets/OS_pic/Ch39.png" width="1000" height="270" /></p>
  </li>
  <li>Removing Files
    <ul>
      <li>unlink() ?</li>
    </ul>
  </li>
  <li>Making Directories
    <ul>
      <li>To create a directory, a single system call, mkdir(), is available. The eponymous mkdir program can be used to create such a directory.</li>
    </ul>
  </li>
  <li>Reading Directories
    <ul>
      <li>the program uses three calls, opendir(), readdir(), and closedir()</li>
      <li>struct dirent</li>
    </ul>
  </li>
  <li>Deleting Directories
    <ul>
      <li>rmdir()</li>
    </ul>
  </li>
  <li>Hard Links
    <ul>
      <li>he link() system call takes two arguments, an old pathname and a new one; when you “link” a new file name to an old one, you essentially create another way to refer to the same file</li>
      <li>The way link() works is that it simply creates another name in the directory you are creating the link to</li>
      <li>reference count (link count)</li>
    </ul>
  </li>
  <li>Symbolic Links
    <ul>
      <li>Soft Link</li>
      <li>the original file can now be accessed through the file name file as well as the symbolic link name file2.</li>
      <li>dangling reference possibility</li>
    </ul>
  </li>
  <li>Permission Bits And Access Control Lists
    <ul>
      <li>permission bits</li>
      <li>what the owner of the file can do to it, what someone in a group can do to the file, and finally, what anyone (sometimes referred to as other) can do.</li>
      <li>chmod command</li>
      <li>form of an access control list (ACL) per directory</li>
    </ul>
  </li>
  <li>Making And Mounting A File System
    <ul>
      <li>What mount does, quite simply is take an existing directory as a target mount point and essentially paste a new file system onto the directory tree at that point</li>
      <li>mkfs</li>
    </ul>
  </li>
</ul>]]></content><author><name>JSC</name><email>newwin0198@handong.ac.kr</email></author><category term="OS" /><category term="OS" /><summary type="html"><![CDATA[OS Ch.39]]></summary></entry></feed>