<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-07-13T23:14:47+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">JSC’s DevLog</title><subtitle>SE and AI version 1.0</subtitle><author><name>JSC</name><email>newwin0198@handong.ac.kr</email></author><entry><title type="html">Clean Code Chapter 2</title><link href="http://localhost:4000/recent/Clean_Code_Ch2/" rel="alternate" type="text/html" title="Clean Code Chapter 2" /><published>2023-07-13T00:00:00+09:00</published><updated>2023-07-13T00:00:00+09:00</updated><id>http://localhost:4000/recent/Clean_Code_Ch2</id><content type="html" xml:base="http://localhost:4000/recent/Clean_Code_Ch2/"><![CDATA[<p><strong>Using the appropriate Name will pay off in the short term and continue to pay in the long run</strong></p>

<ul>
  <li>Using Intention-Revealing Names
    <ul>
      <li>Choosing good names takes time but saves more than it takes</li>
      <li>Choosing a name that specifies what is being measured and the unit of that measurement</li>
      <li>Things need to think
        <ul>
          <li>What kind are in</li>
          <li>What is returned</li>
          <li>zeroth subscript and value</li>
        </ul>
      </li>
      <li>Do not use <strong>O <em>and l</em></strong></li>
    </ul>
  </li>
  <li>Avoid Disinformation
    <ul>
      <li>Avoiding words whose entrenched meanings vary from intention</li>
      <li>Spelling similar concepts similarly
        <ul>
          <li>No inconsistent spelling</li>
        </ul>
      </li>
      <li><del>Using different font??</del></li>
    </ul>
  </li>
  <li>Meaningful Distinction
    <ul>
      <li><strong>a</strong> for a local variable, <strong>the</strong> for all function arguments</li>
      <li>Less redundant words</li>
      <li>Distinguishing names in a way that readers know what the difference offer</li>
    </ul>
  </li>
  <li>Using Pronounceable Names
    <ul>
      <li>Essential for communication</li>
    </ul>
  </li>
  <li>Using Searchable Names
    <ul>
      <li>Single-letter names can ONLY be used as local variables inside short methods</li>
      <li>The length of a name should correspond to the size of its scope</li>
    </ul>
  </li>
  <li>Avoiding Encoding</li>
  <li>Hungarian Notation
    <ul>
      <li>The compiler knows the type nowadays</li>
    </ul>
  </li>
  <li>Member Prefixes
    <ul>
      <li>unseen clutter and a marker of older code…</li>
    </ul>
  </li>
  <li>Interface and Implementation
    <ul>
      <li>Why would I let user knows I’m handling them with an interface?</li>
      <li>Marking at the implementation</li>
    </ul>
  </li>
  <li>Avoid Mental Mapping
    <ul>
      <li>In most contexts, a single letter for a loop is a poor choice (such as i, j, k)</li>
      <li><em>Clarity is king</em></li>
    </ul>
  </li>
  <li>Class Names
    <ul>
      <li>should have noun or noun phrase names</li>
    </ul>
  </li>
  <li>Method Names
    <ul>
      <li>should have verb or verb phase names</li>
      <li>When constructors are overloaded, use the static method with names that describe the argument
        <ul>
          <li>Make corresponding constructors private</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Don’t be cute
    <ul>
      <li>No too “user-specific name”</li>
    </ul>
  </li>
  <li>Pick One Word per Concept
    <ul>
      <li>Pick one word for one abstract concept</li>
      <li>The name will express two different types of object</li>
    </ul>
  </li>
  <li>Don’t Pun
    <ul>
      <li>Avoiding using the same word for two purposes</li>
      <li>Even if it has a similar concept, it must have a different name if it is semantically different</li>
      <li><strong><em>It is the author’s responsibility</em></strong></li>
    </ul>
  </li>
  <li>Use Solution Domain Names
    <ul>
      <li>People who read your code will be programmers
        <ul>
          <li>CS terms, algorithm names, pattern names, math terms, etc.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Use Problem Domain Names
    <ul>
      <li>Separating solution and problem domain concept</li>
    </ul>
  </li>
  <li>Add Meaningful Context
    <ul>
      <li>Place the name in context for the reader
        <ul>
          <li>enclosing classes, functions, or namespaces</li>
        </ul>
      </li>
      <li>break it into smaller functions if it is necessary</li>
    </ul>
  </li>
  <li>But Don’t Add Gratuitous Context
    <ul>
      <li>Shorter names are generally better
        <ul>
          <li>As long as they are clear</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>JSC</name><email>newwin0198@handong.ac.kr</email></author><category term="Books" /><category term="Blog" /><summary type="html"><![CDATA[How to name clearly]]></summary></entry><entry><title type="html">Can We Automatically Fix Bugs by Learning Edit Operations?</title><link href="http://localhost:4000/recent/Can-We-Automatically-Fix-Bugs-by-Learning-Edit-Operations/" rel="alternate" type="text/html" title="Can We Automatically Fix Bugs by Learning Edit Operations?" /><published>2023-07-13T00:00:00+09:00</published><updated>2023-07-13T00:00:00+09:00</updated><id>http://localhost:4000/recent/Can%20We%20Automatically%20Fix%20Bugs%20by%20Learning%20Edit%20Operations</id><content type="html" xml:base="http://localhost:4000/recent/Can-We-Automatically-Fix-Bugs-by-Learning-Edit-Operations/"><![CDATA[<h2 id="can-we-automatically-fix-bugs-by-learning-edit-operations">“Can We Automatically Fix Bugs by Learning Edit Operations?”</h2>

<h2 id="paper"><a href="https://www.cs.wm.edu/~denys/pubs/SANER-RENE-BugFixing.pdf">Paper</a></h2>

<h3 id="summary">Summary:</h3>

<ul>
  <li>
    <p>Implementing Hephaestus, a novel method to improve the accuracy of APR through learning to apply edit operations. Leverages neural machine translation and attempts to produce the edit operations needed.</p>

    <p>Learning edit operations does not offer an advantage over the standard approach of translating directly from buggy code to fixed code. However, interestingly, Hephaestus exhibited lower translation accuracy than the baseline, able to perform successful bug repair.</p>
  </li>
</ul>

<h3 id="points">Points:</h3>

<ol>
  <li>Introduction
    <ol>
      <li>The naive approach attempts some sort of comparison algorithm that identifies the type of bug and replaces it with a prescribed bugs
        <ol>
          <li>Time Consuming</li>
        </ol>
      </li>
      <li>Learning approach using neural machine translation</li>
      <li>Directly applying the NMT approach to source code is inefficient
        <ol>
          <li>Many bugs fixes involve changes to a few sentences
            <ol>
              <li>Results in suboptimal performances</li>
            </ol>
          </li>
          <li>Attempt to mitigate the inefficiency by predicting the specific statement on AST</li>
          <li>Attempting on individual tokens would be more optimal</li>
        </ol>
      </li>
      <li>Hephaestus leverages NMT to predict edit operation, derived from Levenshtein Distance Algorithm
        <ol>
          <li>Working at the token level of source code</li>
          <li>Work on any language without language-specific parsers</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>Related Works
    <ol>
      <li>Tufano et al.
        <ol>
          <li>Repairing code through identification of bug-fix patterns in large software repositories</li>
          <li>Usage of Deep Learning Approach regarding “meaningful” change</li>
        </ol>
      </li>
      <li>Chen et al.
        <ol>
          <li>Focus on single-line bug</li>
        </ol>
      </li>
      <li>Jiang et al.
        <ol>
          <li>The correct fix for a given bug does not exist within the model’s output space and the model’s lack of awareness of syntax</li>
          <li>Pre-train model on the programming language in question</li>
        </ol>
      </li>
      <li>Yuan and Banzhaf
        <ol>
          <li>grouping fine-granularity edits into larger statement-level edits</li>
        </ol>
      </li>
      <li>Mousavi et al.
        <ol>
          <li>Overfitting and Disparity between predicted bug and fix operation and would mimic a human software developer</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>Background
    <ol>
      <li>Fixing buggy code to fixed code using traditional language translation matter of the buggy to fix a variety of language</li>
      <li>Traditional translation replaces the majority of the input sequence which is natural language.
        <ol>
          <li>Fix in code might be minimal</li>
          <li>repair translation should not have the same meaning as the input</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>Approach
    <ol>
      <li>Levenshtein Edit Operation
        <ol>
          <li>the bug is input sequence, the NMT model attempts to produce edit operations</li>
          <li>Basic Operations
            <ol>
              <li>Insertion</li>
              <li>Deletion</li>
              <li>Replacement</li>
            </ol>
          </li>
        </ol>
      </li>
      <li>Compound Edit Operations
        <ol>
          <li>Group of one or more edit operations; sequence of operations</li>
          <li>Condensing: A grouping process to compound operations</li>
        </ol>
      </li>
      <li>Dataset Construction
        <ol>
          <li>Control Dataset: baseline, not involved with edit operations</li>
          <li>Machine String: In order to include edit operations, transforming edit operations
            <ol>
              <li>Typed</li>
              <li>General</li>
            </ol>
          </li>
          <li>We make the distinction between typed and general form to determine if the form of machine string used during training affects the Hephaestus models’ abilities to learn edit operations.</li>
        </ol>
      </li>
      <li>Experimental Dataset
        <ol>
          <li>translate the bug into its corresponding fix, showing Levenshetein edit distance between the bug and fix</li>
          <li>all basic compound operation sequences which transform the bug into the fix, strict is the minimal sequence of the strict compound operation sequences, and loose is the minimal sequence of the loose compound operation sequences</li>
        </ol>
      </li>
      <li>Model Construction
        <ol>
          <li>LSTM+General</li>
          <li>GRU+General</li>
          <li>LSTM+Typed</li>
        </ol>
      </li>
      <li>The CEC ensures that error signals fed forward into the LSTM layers and backpropagated to the LSTM layers are resistant to the effects of the vanishing gradient problem.</li>
    </ol>
  </li>
  <li>Experimental Design
    <ol>
      <li>Perfect Prediction Accuracy</li>
      <li>Failed Prediction Rate</li>
      <li>Edit Distance Decrease</li>
      <li>Training Accuracy</li>
    </ol>
  </li>
  <li>Result
    <ol>
      <li>PPA: The control model (baseline model) outperformed the rest, with no much difference</li>
      <li>FPR: The control model maintained 100% capability, the string can always be interpreted as a sequence of Java method tokens</li>
      <li>EDD: every model generates “bug fixes” that were further away from the fixed code than the original buggy code</li>
      <li>Training Accuracy: Every model exceeded 90%</li>
    </ol>
  </li>
  <li>RQ
    <ol>
      <li>RQ1: Is learning edit operations an effective approach to automatic bug repair?
        <ol>
          <li>learning edit operations does not offer advantages over the baseline approach. The experimental Hephaestus models must determine a sequence of edit operations, decode them, and apply them to the inputted buggy method in order to predict fixed source code</li>
        </ol>
      </li>
      <li>RQ2: What effect does each condensing strategy and machine string form have on the accuracy of bug repair?
        <ol>
          <li>The differences in PPA between the basic, strict, and loose models are negligible, but there are differences according to the training accuracy and average EDD values. Despite having significantly lower final training accuracy, the strict and loose models had slightly more positive EDD values than the basic models (a difference of about 0.96). Thus, it is evidenced that condensing edit operations into strict and loose forms are beneficial over not condensing them at all</li>
        </ol>
      </li>
      <li>RQ3: What is the effect of using an LSTM-based architecture versus a GRU-based architecture on the accuracy of bug repair?
        <ol>
          <li>the variation is not meaningful enough to consider as a key difference between the models.</li>
        </ol>
      </li>
      <li>Future Work
        <ol>
          <li>It was determined that most failed predictions were caused by generated indices outside the valid range for a given string. What changes can be made to this model to restrict the prediction range?</li>
          <li>does changing the abstraction method of the training dataset affect this metric?</li>
          <li>Other NLP Tools</li>
          <li>Extra software layers in addition to the methods presented in our study.</li>
        </ol>
      </li>
      <li>Conclusion
        <ol>
          <li>The introduction of these specific methods for training NMTbased systems to learn bug fixes did not provide a benefit to the task</li>
          <li>Edit operations are capable of performing automated bug repair to some degree</li>
        </ol>
      </li>
    </ol>
  </li>
</ol>

<h3 id="knowledge">Knowledge:</h3>

<ul>
  <li>NMT (Neural Machine Translation): <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">Neural machine translation - Wikipedia</a>uses an artificial neural work to predict the likelihood of a sequence of words</li>
  <li>Levenshtein Distance Algorithm: A string metric for measuring the difference between two sequences. This is likely due to the experimental models experiencing higher entropy than the control when making predictions.</li>
</ul>

<h3 id="terminology">Terminology:</h3>

<ul>
  <li>Condensing Strategies:
    <ul>
      <li>Basic Condensing: basic compound operation corresponds with exactly one change</li>
      <li>Loose Condensing: iff the application of its constituent operation is equivalent to the application of some singular op
        <ul>
          <li>Modify a contiguous section of tokens</li>
        </ul>
      </li>
      <li>Strict Condensing: iff it is loosely compatible and every operation is of the same flavor</li>
    </ul>
  </li>
  <li>Machine Strings:
    <ul>
      <li>Typed: f is one of ins, del, or rep, depending on if the flavor of the represented edit operation is insertion, deletion, or replacement, respectively</li>
      <li>General: general form machine strings do not explicitly store the flavor of their represented edit operations</li>
    </ul>
  </li>
</ul>

<h3 id="tool">Tool:</h3>

<p><a href="https://github.com/WM-SEMERU/hephaestus">GitHub - WM-SEMERU/hephaestus</a></p>

<h3 id="questions">Questions:</h3>]]></content><author><name>JSC</name><email>newwin0198@handong.ac.kr</email></author><category term="APR" /><category term="APR" /><summary type="html"><![CDATA[Generalizing the change is helpful?]]></summary></entry><entry><title type="html">Clean Code Chapter 1</title><link href="http://localhost:4000/recent/Clean_Code_Ch1/" rel="alternate" type="text/html" title="Clean Code Chapter 1" /><published>2023-07-13T00:00:00+09:00</published><updated>2023-07-13T00:00:00+09:00</updated><id>http://localhost:4000/recent/Clean_Code_Ch1</id><content type="html" xml:base="http://localhost:4000/recent/Clean_Code_Ch1/"><![CDATA[<ul>
  <li>What is Clean Code?
    <ul>
      <li>Bjarne Stroustrup, Inventor of C++
        <ul>
          <li>Wasted Cycle are inelegant, not pleasing</li>
          <li>Error handling should be complete</li>
          <li>Focused; Each function, class, and module expose a single-minded attitude that remains entirely undistracted, and unpolluted, by the surrounding details</li>
        </ul>
      </li>
      <li>Grady Booch, author of Object Oriented Analysis and Design with Application
        <ul>
          <li>He took a readability perspective</li>
          <li>Well-written purpose</li>
          <li>clean code should clearly expose the tensions in the problem to be solved</li>
          <li>Should be matter-of-fact as opposed to speculative</li>
        </ul>
      </li>
      <li>Dave Thomas, founder of OTI
        <ul>
          <li>Make it easy for other people to enhance it</li>
          <li>Code without test is not clean at all</li>
          <li>values code that is small, code should be literate</li>
        </ul>
      </li>
      <li>Michael Feathers, author of Working Effectively with Legacy Code
        <ul>
          <li>Looks like it was written by someone who cares</li>
          <li>Someone has taken the time to keep it simple and orderly</li>
        </ul>
      </li>
      <li>Ron Jeffries, author of Extreme Programming Installed and Extreme Programming Adventures in C#
        <ul>
          <li>No duplications</li>
          <li>Express all the design ideas
            <ul>
              <li>High expressiveness</li>
            </ul>
          </li>
          <li>Minimize the number of entities
            <ul>
              <li>Find things in a collection</li>
              <li>Wrap particular implementation in a more abstract method</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Ward Cunningham, inventor of Wiki, inventor of Fit …
        <ul>
          <li>read turns out what you expected</li>
          <li>make it look like the language was made for the problem</li>
          <li>It is the programmer that makes the language appear simple</li>
        </ul>
      </li>
      <li>Author: <a href="https://www.google.com/search?q=Robert+C.+Martin&amp;stick=H4sIAAAAAAAAAONgVuLWz9U3MDSqMspOT3rEaMYt8PLHPWEp3UlrTl5jVOfiCs7IL3fNK8ksqRSS5GKDsvileLmQ9fEsYhUIyk9KLSpRcNZT8E0sKsnMAwD4hk_pWwAAAA&amp;sa=X&amp;ved=2ahUKEwjDuvb5qYSAAxWJAt4KHYeWALQQzIcDKAB6BAgiEAE">Robert Cecil Martin</a>
        <ul>
          <li>Discovering new techniques and founding their own schools</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>Many Recommendations in this book are controversial.</p>

<p>And we are <strong>AUTHORS</strong>.</p>

<ul>
  <li>Making it Easy to read makes it easier to write</li>
  <li>The code has to be kept clean over time</li>
</ul>

<p>This book cannot promise to make you a good programmer.</p>

<p><strong>“Practice, son. Practice!”</strong></p>]]></content><author><name>JSC</name><email>newwin0198@handong.ac.kr</email></author><category term="Books" /><category term="Blog" /><summary type="html"><![CDATA[What is Clean Code?]]></summary></entry><entry><title type="html">Automatic Patch Generation with Context-based Change Application</title><link href="http://localhost:4000/recent/ConFix/" rel="alternate" type="text/html" title="Automatic Patch Generation with Context-based Change Application" /><published>2023-07-10T00:00:00+09:00</published><updated>2023-07-10T00:00:00+09:00</updated><id>http://localhost:4000/recent/ConFix</id><content type="html" xml:base="http://localhost:4000/recent/ConFix/"><![CDATA[<h3 id="automatic-patch-generation-with-context-based-change-application">Automatic Patch Generation with Context-based Change Application</h3>

<h2 id="paper"><a href="https://raw.githubusercontent.com/wiki/thwak/ConFix/pre-print.pdf">Paper</a></h2>

<p>Jindae Kim, Sunghun Kim</p>

<h3 id="goal">Goal:</h3>

<ol>
  <li>I am collecting changes from human-written patches for new patch candidate generation.</li>
  <li>Automatic patch generation technique leveraging human-written patches with our context-based change application technique used by ConFix.</li>
</ol>

<h3 id="summary">Summary:</h3>

<ul>
  <li>An effective patch generation technique should have a large search space with a high probability that patches for bugs are included, and it also needs to locate such patches effectively. Confix collects abstract AST changes from human-written patches, providing resources for patch generation. Then, using only matching context only, Confix selects a necessary change for a possible fixed location. Also, Confix filters out undesirable locations to fix using the info that the location has not been changed in human-written patches.</li>
</ul>

<h3 id="points">Points:</h3>

<ol>
  <li>The problem with APR is search space size and navigation.
    <ol>
      <li>To solve the problem, mining changes from human-written patches has been trying, but it makes navigation for the patch more difficult since the changes are very sparse.</li>
      <li>Confix, use context-based change application technique to generate patch candidates.
        <ol>
          <li>AST context applies collected change to a possible fixed location only if their AST contents are matched</li>
          <li>ConFix compares AST contexts defined by parent, left, and right nodes of a change and a location and applies the change to a target location only if their contexts are matched.</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>Change Collection
    <ol>
      <li>Collecting changes generally applicable to other location</li>
      <li>We extract AST form since source code can be easily affected by specific user styles.</li>
      <li>Following some steps, dependent on how changes are collected</li>
    </ol>
  </li>
  <li>Change Extraction
    <ol>
      <li>Each hunk is extracted and converted to an individual AST subtree change.</li>
      <li>Collect separate individual changes rather than the whole patch to use repetitiveness of changes.</li>
      <li>To extract changes, use the differencing tool, generates AST subtree edit operation, combine node edit operation into trees
        <ol>
          <li>AST node type and node value indicate a specific identifier, literals, or operators; the edit operation preserves the AST structure of the inserted code fragment</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>Change conversion
    <ol>
      <li>Extracted edit operations need to be independent and applicable form.
        <ol>
          <li>The type needs to be changed as a single change
            <ol>
              <li>divide operation more</li>
              <li>extends operation</li>
              <li>remove any operation if a changed AST is a subtree of another operation’s changed AST</li>
              <li>Find all inserted-delete pairs applied to the same location and combine them.</li>
            </ol>
          </li>
        </ol>
      </li>
      <li>Need to normalize the user-defined identifiers.
        <ol>
          <li>By normalizing user-defined names, we can increase the reusability of collected changes since we can apply a change regardless of the availability of user-defined names.</li>
          <li>The technique normalizes user-defined names with two principles: consistency and order-preserving</li>
          <li>By normalizing user-defined names, we can increase the reusability of collected changes since we can apply a change regardless of the availability of user-defined names.</li>
          <li>At the time of change concretization, the collected information works as requirements for each abstract name which ConFix should consider when it assigns a concrete name for the abstract name.</li>
        </ol>
      </li>
      <li>Change Context Identification
        <ol>
          <li>After individual changes are obtained from source code patches, the next step is identifying the AST contexts of the changes.</li>
          <li>By identifying the context of the target location, we can avoid meaningless modifications and more frequent changes.</li>
          <li>To define AST context, use nearby nodes of a changed AST subtree
            <ol>
              <li>The parent node represents the location where a changed code fragment belongs.</li>
              <li>The left and Right nodes indicate code fragments before and after the changed code fragment.</li>
              <li>Using fingerprinting technique</li>
              <li>Dyck Word Hash
                <ol>
                  <li>represents the parent-child relationship of nodes</li>
                  <li>Compare two nodes’ hash values to check whether they are the roots of two type-isomorphic AST subtrees.</li>
                  <li>For fast comparison</li>
                </ol>
              </li>
            </ol>
          </li>
          <li>PLRTH and PLRT
            <ol>
              <li>left node indicates code before the changed code frame; the right node indicates code after the changed code fragment</li>
              <li>PTLRH context, we also examine whether the change and the location have the same code fragment before and after them, while PLRT context only checks which kind of code fragments exist before and after the change and the location.</li>
              <li>PTLRH context constrains the search area too much; we can expand the search area with PLRT context.</li>
              <li>Exception</li>
              <li>Block node does not mean too much; use its parent node as a type.</li>
              <li>Move operations primarily consider the old location’s context, but store the new location’s context to use it as an additional constraint of move changes.</li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>
  </li>
  <li>Change Application
    <ol>
      <li>Target Location Context Identification
        <ol>
          <li>The target location is also an AST node to which a change will be applied</li>
          <li>To identify the location context is compatible with change contexts, using PLR nodes we can identify location contexts for changes
            <ol>
              <li>Except for insert, another operation has an old AST subtree so it is applicable</li>
              <li>The inserted context is identified from New AST which is inserted. We need a different type, based on the assumption that a subtree is inserted near a location. Insert Before and Insert After contexts are the contexts for cases in which a new AST subtree is inserted before and after node N. Node C indicates the actual location where a subtree will be inserted</li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>
  </li>
  <li>Change Selection
    <ol>
      <li>retrieve changes with the same context from a changing pool</li>
      <li>Categorized by their context, only store unique changes with their frequency</li>
      <li>choose one of the selections and mimic human-written changes by selecting any of the changes on the list
        <ol>
          <li>The random selection, strategy is not decided specifically</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>Change Concretiziation
    <ol>
      <li>Replacing all normalized identifiers with concrete name
        <ol>
          <li>Do not fully specify the strategy yet how ConFix concretizes a change since various strategies can be used</li>
          <li>Since the type of normalized variables and signature of normalized methods are also stored during change collection, we can consider a strategy that assigns an existing variable name to a normalized variable (if the type is compatible)</li>
          <li>Since change is concretized, apply it to the target location</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>Patch generation
    <ol>
      <li>Generate-and-validate process</li>
      <li>Starts with the PTLRH pool to narrow down the search area for patch generation, switch to PLRT if the patch generation failed at the above level</li>
      <li>Identify target location, retrieves changes having the same context as the selected location, apply it to the target location</li>
      <li>Lastly, tries different name assignments by predefined max trials
        <ol>
          <li>To prevent spoiled validation due to wrong name assignment</li>
        </ol>
      </li>
      <li>Pass -&gt; termination; Fail -&gt; continues with another change</li>
      <li>When it reaches the max candidate, moves to the next change pool</li>
    </ol>
  </li>
  <li>Target Location Identification
    <ol>
      <li>Confix leverages both fault localization and change context to identify target locations from given buggy code
        <ol>
          <li>Identifies all AST nodes which belong to the statement as potential target locations</li>
          <li>filters out all target locations having context not included in the current change pool</li>
          <li>Identify fail and pass a group
            <ol>
              <li>The first pick from the fail-only group, if there is no location, starts from the pass-fail group</li>
              <li>Failing test cases have a much higher priority</li>
            </ol>
          </li>
          <li>Prior target location has if, method invocation, infix expression, or return statement since it influences the whole execution</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>Change Selection
    <ol>
      <li>ConFix selects one of the retrieved changes for the current target location and chooses the most frequent change</li>
      <li>Even if it is in the same context, change might not be applicable
        <ol>
          <li>In case of replacement operation, one more location should be selected</li>
          <li>Therefore, ConFix randomly selects one of the target locations with a matching context
            <ol>
              <li>If there is not, discard the change</li>
            </ol>
          </li>
        </ol>
      </li>
      <li>Code might not be compilable, so ConFix goes through the verification steps
        <ol>
          <li>Examine all change-location pairs since a change that was applicable in another location might not be applicable in other locations</li>
          <li>Considering all pairs and compiling is beneficial rather than since it is not expensive to recompile source code with a small modification</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>Change Concretization
    <ol>
      <li>Concretizing selected change for the target location</li>
      <li>ConFix collects concrete names of variables, types, and methods from the given buggy code and decides which names are within the scope</li>
      <li>To find a concrete method for a normalized method, ConFix identifies compatible methods from collected methods available at a target location</li>
      <li>Abstract signature
        <ol>
          <li>method signature with normalized types</li>
          <li>The purpose of this is to find a concrete method with the same abstract signature, then it will not cause a compile error</li>
        </ol>
      </li>
      <li>Assignable type
        <ol>
          <li>Consider both type compatibility and the number of variables of the types
            <ol>
              <li>Normalized types are considered wildcard characters, which means that they can be assignable to either normalized types or JSL types.</li>
              <li>ConFix only considers a type as assignable to another type if there exist enough variables for assignment.</li>
            </ol>
          </li>
          <li>the compatibility of a normalized and a concrete method is defined by their abstract signatures and assignable types
            <ol>
              <li>The concrete method first considers local methods, then global methods with the assumption that local methods are more closely related to buggy code</li>
              <li>Once a concrete method for a normalized method is selected and assigned, ConFix update types are matched due to method assignment</li>
              <li>Randomly assigns one of the assignable concrete types for each normalized type</li>
              <li>Assigns concrete variables to normalized variables</li>
            </ol>
          </li>
          <li>There is one additional treatment for update type changes which updates an identifier with another identifier
            <ol>
              <li>Assume the update change is meaningful when the new name is similar to the old one, so calculate the Levenshtein distance between the identifier and concrete name</li>
            </ol>
          </li>
          <li>In case of no type-compatible assignment
            <ol>
              <li>Change concretization is considered failed and no patch candidate is generated</li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>
  </li>
  <li>Evaluation
    <ol>
      <li>Collected changes with PTLRH and PTLR contexts from Apache Commons Collections (collections), Derby (derby), Hadoop (hadoop), Ivy (ivy) and Lucene (lucene) projects.
        <ol>
          <li>we selected a completely different set of projects for change resources from the projects in Defects4j dataset.</li>
          <li>For each context, there are averages 1.91 and 16.25 changes in PTLRH and PLRT change pools respectively</li>
        </ol>
      </li>
      <li>We built change pools and obtained coverage information of all buggy codes before we applied ConFix to each bug.
        <ol>
          <li>Change pool and coverage information is given to ConFix</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>Results
    <ol>
      <li>Compilation of one Java file is much cheaper than test execution even if only a few failing test is executed. Therefore ConFix can find a patch within a reasonable time.</li>
      <li>Acceptable and Plausible</li>
      <li>We did not set a time budget, however, ConFix generated all the patches within two hours</li>
      <li>ConFix generated 71 patches
        <ol>
          <li>which are greater than other techniques - ssFix(60), Nopol(33), jGenProg(19), jKali(18), HDRepair(16) and ACS(7)</li>
          <li>generated 13 acceptable patches, which is significantly higher than valid patches generated by HDRepair(5), ACS(3), jGenProg(3), jKali(1), and Nopol(0). One exception is ssFix, which generated 20 valid patches, higher than ConFix.</li>
        </ol>
      </li>
      <li>ssFix
        <ol>
          <li>We verified ssFix-generated patches again and found that two valid patches (C1, M79) are not acceptable patches.</li>
        </ol>
      </li>
      <li>ConFix was able to find necessary changes and the right fix locations with its patch generation strategy</li>
      <li>Informative Patches
        <ol>
          <li>Although plausible patches are not acceptable when it is compared to human-written patches, they might be given as debugging hints for developers.</li>
          <li>we manually analyzed 58 plausible patches and checked whether these patches are informative.</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>Are PLRTH and PLRT really helpful?
    <ol>
      <li>With PTLRH contexts, ConFix explores a much narrower area in its candidate space
        <ol>
          <li>Consequently, it might also lose the chance that actual patches are included in the area</li>
        </ol>
      </li>
      <li>Among 71 generated patches, 81% of the patches (58/71) are generated by changes from PTLRH change pools. In terms of acceptable patches, using PLRT context only gives two more acceptable patches, and PTLRH change pool still works for 85% of the acceptable patches (11/13).</li>
      <li>ConFix generated two acceptable patches which take 15% of all acceptable patches under both types of context. These PLRT acceptable patches are all semantically equivalent to human-written patches, which means that they addressed issues in the same way as humans did. Therefore PLRT context can also provide practical constraints to mimic the developer’s changes and produce acceptable patches.</li>
    </ol>
  </li>
  <li>Threats to validity
    <ol>
      <li>Our evaluation results might be different if we used other collected changes from different human-written patches.</li>
      <li>There exists another issue that bugs from five projects might not be representative</li>
      <li>Manual assessment of patches could be another issue, since we do not have domain knowledge and the judgment about patches might be subjective and biased.</li>
    </ol>
  </li>
  <li>Related Works
    <ol>
      <li>The difference is that ConFix uses AST contexts to select one of the changes, while ssFix considers the syntactic relation of code fragments to given buggy code.</li>
      <li>Identifying AST contexts is less costly than deriving SMT constraints, but these contexts still provide syntactic information which also implies some of the program semantics, although it does not fully represent the program’s semantics like SMT constraints.</li>
      <li>ConFix differs from these previous techniques due to the point that it can automatically collect abstract individual changes on a large scale and it uses them to generate patch candidates, instead of generating patch candidates with several pre-defined modifications or mutation operations with limited resources of code fragments</li>
    </ol>
  </li>
  <li>Studies on Human-written patches
    <ol>
      <li>changes in bug fixes are repetitive, and smaller changes are even more repetitive</li>
      <li>There exist other studies on changes and source code’s uniqueness which imply the potential of techniques leveraging 17 existing code fragments or changes</li>
      <li>Empirical evaluation of ConFix and fixability analysis results imply that we can obtain necessary changes for new bug fixes from existing patches.</li>
    </ol>
  </li>
  <li>Change Collection and Application
    <ol>
      <li>we may consider using these code transfer techniques to develop new methods for patch candidate generation in ConFix</li>
      <li>it is possible to apply high-level ideas such as collecting abstract changes with their AST contexts regardless of adjustment</li>
    </ol>
  </li>
  <li>Conclusion
    <ol>
      <li>We may try to generate patches with multiple changes to improve partial patches up to acceptable patches or use more sophisticated concretization methods to effectively generate high-quality patches.</li>
    </ol>
  </li>
</ol>

<h3 id="knowledge">Knowledge:</h3>

<ul>
  <li>Context-based Change Application Technique: a technique to generate candidate patches</li>
  <li>ConFix can expand its search space by collecting more changes, while it can navigate through them effectively with the guidance of context</li>
</ul>

<h3 id="terminology">Terminology:</h3>

<ul>
  <li>Hunk: Single changes including deletion and addition, may have multiple in a single commit</li>
</ul>

<h3 id="questions">Questions:</h3>

<ul>
  <li>Why PLRTH and PLRT: The paper mentioned it is for reducing the search space. Is it the most efficient way to do it?</li>
  <li>I don’t understand the part about fault localization. Is it possible to localize the fault using AST context? Does it just localize fault according to the collected change?</li>
  <li>What if AST differencing tools work maliciously?</li>
</ul>]]></content><author><name>JSC</name><email>newwin0198@handong.ac.kr</email></author><category term="APR" /><category term="APR" /><summary type="html"><![CDATA[ConFix]]></summary></entry><entry><title type="html">알고리즘 분석과 벚꽃</title><link href="http://localhost:4000/recent/Prof.Yong-Algorithm-Analysis/" rel="alternate" type="text/html" title="알고리즘 분석과 벚꽃" /><published>2023-06-09T00:00:00+09:00</published><updated>2023-06-09T00:00:00+09:00</updated><id>http://localhost:4000/recent/Prof.Yong-Algorithm-Analysis</id><content type="html" xml:base="http://localhost:4000/recent/Prof.Yong-Algorithm-Analysis/"><![CDATA[<p>거의 처음으로 한글로 블로그 포스팅을 써보는 것 같다! 
아무래도 한국어가 제일 편하긴하다 ㅎ <br /></p>

<p>블로그 포스팅을 학기 중에 하고자 헀지만 도저히 안되더라… 그래서 학기 중에 블로그 포스팅 할 것들 메모해놓고 방학 중에 일일히 쓰는 중이다! 날짜도 그 때 썼다고 생각하고 업로드 하는 중이기도 하고… ㅋㅋㅋ<br /></p>

<p>한동에서의 3학년이 사망년이 되지 않기를 바라는 포스트 이후 논문 리뷰나 책 리뷰 외의 글을 쓰는 것은 몇달 만인 것 같다. 원래 쓸 생각이 없었는데 포스팅 할 내용이 나에게 큰 위로가 되었기도 하고… <del>랩실에 블로그 괴물이 있다… 진짜 너무 잘씀 꼭 <a href="https://0neand0nly.github.io/">여기</a>가 보세요</del></p>

<p>이번 블로그 글의 제목은 알고리즘 분석과 벚꽃이다! 무슨 관계가 있을까 싶겠지만 꽤나 깊은 관계(?)가 있다.</p>

<p>한동에는 중간고사 기간을 주위로 하여 벚꽃이 핀다! 벚꽃이 정말 아름답게 핀다. 내가 사진을 잘 안찍어서 벚꽃 사진이 없어서 첨부를 못하는게 아쉽다 ㅠㅠ 뭐 찾으면 있겠지만 귀찮기도 하고 사진을 왜 찍지 이해를 못하기도 하고… 뭐 암튼 그렇다. 거두절미하고 이 글은 알고리즘 분석 6월 9일 수업 때 일어난 일의 이야기를 담고 있다. 기억을 더듬어보자면 기말 고사를 앞둔 기간이었고, 과제는 미칠 듯이 쏟아져 나오고 있었다. 다들 너무 지쳐서 수업 분위기는 말이 아니었고 나도 굉장히 지쳐있었다.</p>

<p>수업분위기에 화를 몇 번 내시던 교수님께서 갑자기 벚꽃 이야기를 꺼내셨다. 학교 벚꽃이 이쁘다면서 벚꽃이랑 사진찍는 학생들을 매년 보고 있다는 이야기를 하셨다. 뒤이어 학생들이 벚꽃과 사진을 찍는 것을 이해하지 못한다고 하셨다. 내심 나랑 공감하는 사람이 있다는 생각이 드는 찰나 교수님은 갑자기 닭살돋는 이야기를 하셨다. <em>“이 아이들은 자신이 벚꽃보다 아름답다는 걸 모르나? 왜 사진을 찍지?”</em> 많이 놀랬다. 솔직히 알고리즘 분석 담당 교수님께서 화를 안 내시는 분은 아니라…</p>

<p>그렇게 말씀하시면서 여러 말을 덧붙이셨다. 사람이 가장 아름다운 나이는 20대 초중반라고. 우리가 지금 20대 초중반을 지나고 있기에 우리가 하는 일들은 모두 아름답고 가치 있는 일이라고. 반대로 이렇게 꽃다운 나이기에 치열하게 살아가고, 후회없이 모든 것을 쏟아낼 수 있는 나이라고.</p>

<p>솔직히 많은 위로가 됐다. 내가 글을 쓰는 재주도 없고 한 달 정도가 지난 후에 글을 쓰는 만큼 글에 감동이 하나도 묻어있지 않는게 너무 아쉽다…</p>

<p>이제는 젊다고 할 수 없는 나이에 접어들었지만 치열하게 살아가야 한다는 사실은 오히려 나이가 들수록 깨닫고 있는 것 같다. 나이가 들고 후회하기 보다는 노력해보고 후회하는게 나쁘지 않은 것 같다 ㅎㅎ 교수님 감사합니다!</p>]]></content><author><name>JSC</name><email>newwin0198@handong.ac.kr</email></author><category term="Blog" /><category term="Blog" /><summary type="html"><![CDATA[젊음!]]></summary></entry><entry><title type="html">Operating Systems: Three Easy Pieces Ch. 22</title><link href="http://localhost:4000/recent/Chapter-22/" rel="alternate" type="text/html" title="Operating Systems: Three Easy Pieces Ch. 22" /><published>2023-05-20T00:00:00+09:00</published><updated>2023-05-20T00:00:00+09:00</updated><id>http://localhost:4000/recent/Chapter-22</id><content type="html" xml:base="http://localhost:4000/recent/Chapter-22/"><![CDATA[<p>Beyong Physical Memory: Policies</p>

<ul>
  <li>Cache Management
    <ul>
      <li>cache for virtual memory pages in the system
        <ul>
          <li>Need to minimize cache misses</li>
          <li>Maximize the cache hit</li>
        </ul>
      </li>
      <li>Average memory access time
        <ul>
          <li>cost of disk access is so high in modern systems that even a tiny miss rate will quickly dominate the overall AMAT of running programs.</li>
          <li>AMAT = TM + (PMiss · TD )</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Optimal Replacement Policy
    <ul>
      <li>Optimal policy
        <ul>
          <li>Cold-start Miss (compulsory miss)</li>
          <li>Other than that it would be optimal, but it is impossible to build the optimal policy for general-purpose operating system</li>
        </ul>
      </li>
      <li>FIFO
        <ul>
          <li>first-in, first-out replacement</li>
        </ul>
      </li>
      <li>Random
        <ul>
          <li>Random page to replace under memory pressure</li>
          <li>depends upon how lucky it is</li>
          <li>Must be better than random</li>
        </ul>
      </li>
      <li>LRU
        <ul>
          <li>Using historical information: recency</li>
          <li>Using the principle of locality</li>
          <li>Replace least-recently-used</li>
        </ul>
      </li>
      <li>LFU
        <ul>
          <li>using historical information: frequency</li>
          <li>using the principle of locality</li>
          <li>Replace least-frequently-used</li>
        </ul>
      </li>
      <li>Other
        <ul>
          <li>MFU</li>
          <li>MRU</li>
          <li>do not work well, ignore the locality</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Workload
    <ul>
      <li>No locality
        <ul>
          <li>when there is no locality in the workload, it doesn’t matter much which realistic policy you are using; LRU, FIFO, and Random all perform the same, with the hit rate exactly determined by the size of the cache. Second, when the cache is large enough to fit the entire workload, it also doesn’t matter which policy you use;</li>
        </ul>
      </li>
      <li>With the locality(80-20 locality)
        <ul>
          <li>LRU does better, as it is more likely to hold onto the hot pages; as those pages have been referred to frequently in the past, they are likely to be referred to again in the near future</li>
        </ul>
      </li>
      <li>Looping-Sequential Workload
        <ul>
          <li>pages have been referred to frequently in the past, they are likely to be referred to again in the near future. Optimal once again does better, showing that LRU’s historical information is not perfect</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Historical algorithm
    <ul>
      <li>LRU requires additional things and works</li>
      <li>One solution → time keeper hardware, but it is not desirable for huge computer system</li>
      <li>Approximating LRU → currently using
        <ul>
          <li>use a bit (reference bit)</li>
          <li>Clock algorithm (a circular list)
            <ul>
              <li>The clock hand points to some particular page</li>
              <li>When the replacement must occur, check that currently-pointed to page P has a useful bit of 1 or 0</li>
              <li>The algorithm continues until it finds a use bit that is set to 0, implying this page has not been recently used</li>
              <li>nice property of not repeatedly scanning through all of the memory looking for an unused page.</li>
            </ul>
          </li>
          <li>scan resistance</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Dirty pages
    <ul>
      <li>modified bit or dirty bit
        <ul>
          <li>scan for unused and clean pages to evict first; fail to find those, then for unused pages that are dirty, and so forth.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Page selection policy
    <ul>
      <li>Demand paging → OS brings the page into memory when it is accessed, “on-demand”</li>
      <li>OS could guess that a page is about to be used, and thus bring it in ahead of time; this behavior is known as prefetching</li>
    </ul>
  </li>
  <li>Os writes pages out to disk
    <ul>
      <li>Collect a number of pending writes and writes at once</li>
      <li>Clustering or grouping of writes</li>
    </ul>
  </li>
  <li>Trashing
    <ul>
      <li>When the demands exceed the available memory space</li>
      <li>Early system → admission control
        <ul>
          <li>could decide not to run the subset, with the hope of reducing set of the process working sets fit in memory and thus can make the process</li>
        </ul>
      </li>
      <li>Current
        <ul>
          <li>out-of-memory killer
            <ul>
              <li>memory is oversubscribed, chooses a memory-intensive process and kills it</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>JSC</name><email>newwin0198@handong.ac.kr</email></author><category term="OS" /><category term="OS" /><summary type="html"><![CDATA[OS Ch.22]]></summary></entry><entry><title type="html">Operating Systems: Three Easy Pieces Ch. 21</title><link href="http://localhost:4000/recent/Chapter-21/" rel="alternate" type="text/html" title="Operating Systems: Three Easy Pieces Ch. 21" /><published>2023-05-20T00:00:00+09:00</published><updated>2023-05-20T00:00:00+09:00</updated><id>http://localhost:4000/recent/Chapter-21</id><content type="html" xml:base="http://localhost:4000/recent/Chapter-21/"><![CDATA[<p>Beyound Physical Memory: Mechanisms</p>

<ul>
  <li>Memory Hierarchy → additional level, if OS needs to support large address space, it needs to keep every portion of address spaces even if it is not currently using
    <ul>
      <li>Hard Disk Drive</li>
      <li>Why do we want to support a single large space? It is convenient to use</li>
    </ul>
  </li>
  <li>Memory overlays → programmers to manually move pieces of code or data in and out</li>
  <li>Swap space → swap the spaces, pages out of memory to it and swap pages into memory from it
    <ul>
      <li>Need to remember disk address</li>
      <li>We should note that swap space is not the only on-disk location for swapping traffic.</li>
      <li>Safely re-use the memory space for the file that is in the disk storage</li>
    </ul>
  </li>
  <li>Present Bit
    <ul>
      <li>Running process generates virtual memory references → the hardware translates them into physical addresses before fetching the desired data from memory
        <ul>
          <li>TLB hit, TLB miss</li>
          <li>PTE</li>
        </ul>
      </li>
      <li>If we need to allow pages to be swapped to disk → need to indicate the page is present in the physical memory or not</li>
      <li>it is set to zero, the page is not in memory but rather on disk somewhere. The act of accessing a page that is not in physical memory is commonly referred to as a page fault</li>
    </ul>
  </li>
  <li>Page fault
    <ul>
      <li>OS is in charge to handle the page fault → page-fault handler
        <ul>
          <li>Hardware page-fault handler</li>
          <li>Software page-fault handler</li>
        </ul>
      </li>
      <li>OS needs to swap the page into memory → how?
        <ul>
          <li>the page table is a natural place to store such information. Thus, the OS could use the bits in the PTE normally used for data such as the PFN of the page for a disk address.</li>
          <li>Disk I/O → update the page table → TLB Miss → Bring up the page to TLB</li>
          <li>The process will be blocked state so OS will be free to run</li>
          <li>Overlap of I/O of one process and execution of another way to the effective usage of its hardware</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Page-replacement policy
    <ul>
      <li>Page in, Page out</li>
      <li>Replacing, cost a lot</li>
    </ul>
  </li>
  <li>Three cases
    <ul>
      <li>Page was both present and valid → TLB miss handler grap PFN from PTE, retry the instruction</li>
      <li>Page is valid but not present, Page fault handler → when the page is legitimate</li>
      <li>Invalid page access → bug, trap the invalid access, terminating</li>
    </ul>
  </li>
  <li>OS to keep a small portion of memory free more proactively
    <ul>
      <li>High watermark</li>
      <li>Low watermark
        <ul>
          <li>OS notice fewer than LW page available, the background thread is responsible for freeing memory runs</li>
          <li>evicts the page until there are HW pages available</li>
        </ul>
      </li>
      <li>Swap daemon
        <ul>
          <li>Many systems cluster or group a number of pages and write them out at once to the swap partition</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>these actions all take place transparently to the process</li>
  <li>Replacement occurs
    <ul>
      <li>high watermark (HW ) and low watermark (LW ) to help decide when to start evicting pages from memory</li>
      <li>systems will cluster or group a number of pages and write them out at once to the swap partition</li>
    </ul>
  </li>
</ul>]]></content><author><name>JSC</name><email>newwin0198@handong.ac.kr</email></author><category term="OS" /><category term="OS" /><summary type="html"><![CDATA[OS Ch.21]]></summary></entry><entry><title type="html">OS-HW2 Summary</title><link href="http://localhost:4000/recent/OS-HW2/" rel="alternate" type="text/html" title="OS-HW2 Summary" /><published>2023-05-12T00:00:00+09:00</published><updated>2023-05-12T00:00:00+09:00</updated><id>http://localhost:4000/recent/OS-HW2</id><content type="html" xml:base="http://localhost:4000/recent/OS-HW2/"><![CDATA[<h1 id="hw-2">HW 2</h1>

<p>First of all, it was the first assignment that I have not completed since I have start Computer Science Major… I slept 11 hours for 4 days.</p>

<ul>
  <li>Delta debugging:
    <ul>
      <li>a test input minimization technique that takes a target program together with a crashing test input, and then automatically derives a part of the test input that still makes the program crash.</li>
      <li>This algorithm receives three inputs: (1) 𝑝, an executable binary of a target program, (2) 𝑡, a test input that crashes the target program, and (3) 𝑐, a condition to determine the crash.</li>
    </ul>
  </li>
  <li>
    <p>Overall requirement</p>

    <p>it checks if the regions of size 𝑠 (Line 13) crash 𝑝 while preserving the crash behavior. If found, the algorithm updates 𝑡𝑚 and continues the reduction with it (Lines 14-17). If the algorithm could not find a reduced crashing input with 𝑠, it decrements s by one (Line 19), and repeats the input subsequence exploration until 𝑠 becomes zero (Line 3)</p>

    <ul>
      <li>User interface:
        <ul>
          <li>must return an error message if a given argument is invalid</li>
          <li>option arguments
            <ul>
              <li>I option: receive full path of the crashing input</li>
              <li>m option: string appeared in the standard error</li>
              <li>o option: new file to store the reduced crashing input</li>
            </ul>
          </li>
          <li>After the option arguments
            <ul>
              <li>executable binary of the target program</li>
              <li>If the target program needs to receive its own command-line arguments, multiple arguments must be given.</li>
            </ul>
          </li>
          <li>User interrupt
            <ul>
              <li>Ctrl+C: for this case, cimin must stop the running test if exists, and prints out the size of the shortest crashing input found so far, produces the output with it, and terminates the execution.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>System Design
        <ul>
          <li>Running the target program
            <ul>
              <li>To run the target program, cimin must use fork() to create a new process, and exec() (or its variants) to load the target program in the created process.</li>
              <li>Assumption: receives input via standard input and sends out crash message to standard error</li>
            </ul>
          </li>
          <li>Using Pipe
            <ul>
              <li>Target program receives input via standard input and sends out crash message to standard error. To pass input to the target program, cimin must use unnamed pipe (i.e., pipe()) to redirect standard input.</li>
              <li>Also, it use to redirect the standard error to receive the error message from the target program execution</li>
              <li>Assumption: We assume that the target program produces crash message to standard error, and we can find a keyword to determine if the same crash happens with reduced inputs.</li>
            </ul>
          </li>
          <li>Using signal
            <ul>
              <li>cimin must reject the initial crashing input if its execution takes more than 3 seconds.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Buggy Programs
        <ul>
          <li>jsmn
            <ul>
              <li>Bug with heap-buffer overflow errors</li>
              <li>LLVM AddressSanitizer → explicitly detect an occurrence of heap-buffer overflow buffer</li>
              <li>You can characterize this crashing message by checking if it has “AddressSanitizer: heap-buffer-overflow”.</li>
            </ul>
          </li>
          <li>libxml2
            <ul>
              <li>Bugs with null pointer deference errors.</li>
              <li>You can trigger this bug by executing xmllint with option “–recover –postvalid -” and passing libxml2/testcases/crash.xml to standard input.</li>
              <li>The crashing symptom can be identified by checking if “SEGV on unknown address” is printed to standard error. This error message is produced by LLVM AddressSanitizer when a crash occurs by null pointer dereference.</li>
            </ul>
          </li>
          <li>balance
            <ul>
              <li>balance is a toy example program that checks if the given input has well-balanced parenthesis or not.</li>
              <li>When this program receives balance/testcases/fail via standard input, it will fall into infinite loop.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Report writing
        <ul>
          <li>Must not exceed 3 pages</li>
          <li>Note that the evaluation is primary based on your report, and your implementation will be tested to check whether it consistently works as described in the report</li>
        </ul>
      </li>
      <li>Submission
        <ul>
          <li>Report → PDF</li>
          <li>Makefile</li>
          <li><a href="http://README.md">README.md</a></li>
          <li>Runnable at the peace.handong.edu</li>
        </ul>
      </li>
    </ul>

    <p>Crash_copy.json</p>

    <p>→ Only with the new line character → no output</p>
  </li>
</ul>]]></content><author><name>JSC</name><email>newwin0198@handong.ac.kr</email></author><category term="OS" /><category term="OS" /><summary type="html"><![CDATA[OS HW 2... Prof. Hong...]]></summary></entry><entry><title type="html">Operating Systems: Three Easy Pieces Ch. 20</title><link href="http://localhost:4000/recent/Chapter-20/" rel="alternate" type="text/html" title="Operating Systems: Three Easy Pieces Ch. 20" /><published>2023-05-12T00:00:00+09:00</published><updated>2023-05-12T00:00:00+09:00</updated><id>http://localhost:4000/recent/Chapter-20</id><content type="html" xml:base="http://localhost:4000/recent/Chapter-20/"><![CDATA[<p>Paging: Smaller Tables</p>

<p>Page tables are too big and thus consume too much memory.</p>

<ul>
  <li>32-bit address space (2^32 bytes), with 4KB (2^12 bytes) and 4-byte page-table entry
    <ul>
      <li>Simple array-based page tables are too big, taking up far too much memory on typical systems</li>
      <li>We are in search of some techniques to reduce</li>
    </ul>
  </li>
  <li>Simple solution: Bigger pages
    <ul>
      <li>Increasing the page size may decrease the page table size</li>
      <li>But internal fragmentation will occur</li>
    </ul>
  </li>
  <li>Hybrid approach: Paging and Segments
    <ul>
      <li>Instead of having a single-page table for the entire address space of the process, why not have one per logical segment?</li>
      <li>We still have MMU → hold the physical address of the page table of that segment</li>
      <li>Three base/bound pairs, one each for code, heap, and tack
        <ul>
          <li>When the process is running, the base register for each of these segments contains the physical address of a linear page table for that segment</li>
          <li>each process in the system now has a three-page table associated with it</li>
          <li>On a context switch, these registers must be changed to reflect the location of the page tables of the newly-running process</li>
        </ul>
      </li>
      <li>On the TLB miss → hardware uses the segment bits to determine which base and bound pair to use
        <ul>
          <li>Hardware takes the physical address and combines it with VPN to form an address of the page table entry</li>
          <li>The critical difference in our hybrid scheme is the presence of a bounds register per segment</li>
          <li>But…
            <ul>
              <li>if we have a sparsely-used heap, we can still end up with a lot of page table waste</li>
              <li>since most of the memory is managed in <strong>page-sized units</strong>, page tables can be of arbitrary size, and finding free space is complicated</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Multi-level page tables
        <ul>
          <li>chop up the page table into page-sized units, then if an entire page of page-table entries is invalid, don’t allocate that page of the page table at all</li>
          <li>Page directory</li>
          <li>The page directory, in a simple two-level table, contains one entry per page of the page table</li>
          <li>consists of a number of page directory entries (PDE)</li>
          <li>A PDE (minimally) has a valid bit and a page frame number (PFN), similar to a PTE.
            <ul>
              <li>But the meaning of valid bit is different, if it is valid, it means at least one of the pages of the page table that the entry points to is valid</li>
            </ul>
          </li>
          <li>the multi-level table only allocates page-table space in proportion to the amount of address space you are using</li>
          <li>each portion of the page table fits neatly within a page, making it easier to manage memory</li>
          <li>level of indirection using the page directory, which points to pieces of the page tables</li>
          <li>In the case of miss
            <ul>
              <li>two loads from memory will be required
                <ul>
                  <li>one for the page directory, one for PTE itself</li>
                </ul>
              </li>
              <li>time-space tradeoff</li>
              <li>complexity</li>
            </ul>
          </li>
          <li>To build a two-level page table for this address space, we start with our full linear page table and break it up into page-sized units.</li>
          <li>As a result, we need four bits of the VPN to index into the directory; we use the top four bits of the VPN</li>
          <li>page-directory entry (PDE) with a simple calculation: PDEAddr = PageDirBase + (PDIndex * sizeof(PDE)).</li>
        </ul>
      </li>
      <li>More than two-levels</li>
      <li>Translation process
        <ul>
          <li>the physical address is formed directly without accessing the page table at all, as before. Only upon a TLB miss does the hardware need to perform the full multi-level lookup.</li>
        </ul>
      </li>
      <li>Inverted Page Tables
        <ul>
          <li>Even more extreme space saving in the world of page tables is found with inverted page tables</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>some systems place such page tables in kernel virtual memory, allowing the system to swap some page tables to disk when memory pressure gets a little tight.</li>
</ul>

<p>In summary, it also introduces increased complexity, potential memory overhead, challenges in address translation and memory protection, and fragmentation concerns, which need to be carefully considered and managed in the design and implementation of such a memory management scheme.</p>

<p>PTE → bit의 순서와 optimization</p>]]></content><author><name>JSC</name><email>newwin0198@handong.ac.kr</email></author><category term="OS" /><category term="OS" /><summary type="html"><![CDATA[OS Ch.20]]></summary></entry><entry><title type="html">Operating Systems: Three Easy Pieces Ch. 17</title><link href="http://localhost:4000/recent/Chapter-17/" rel="alternate" type="text/html" title="Operating Systems: Three Easy Pieces Ch. 17" /><published>2023-04-20T00:00:00+09:00</published><updated>2023-04-20T00:00:00+09:00</updated><id>http://localhost:4000/recent/Chapter-17</id><content type="html" xml:base="http://localhost:4000/recent/Chapter-17/"><![CDATA[<p>Free-Space Management</p>

<p>When free-space management becomes more difficult is when the free space with user-level memory-allocation memory</p>

<ul>
  <li>Segmentation</li>
  <li>External Fragmentation</li>
  <li>Internal fragmentation
    <ul>
      <li>if an allocator hands out chunks of memory bigger than that requested, any unasked for (and thus unused) space in such a chunk</li>
    </ul>
  </li>
  <li>void pointer</li>
  <li>Low-level Mechanisms
    <ul>
      <li>Splitting and Coalescing
        <ul>
          <li>requesting more than space that is free will fail → splitting</li>
          <li>will find a free chunk of memory that can satisfy request</li>
          <li>What if returned space is in the middle of heap?</li>
          <li>It may end up with divided arrays with continuous size</li>
        </ul>
      </li>
      <li>Tracking the size of allocated regions
        <ul>
          <li>header block</li>
          <li>Embedding Free list</li>
          <li>mmap</li>
          <li>Fragmented free spaces
            <ul>
              <li>go through the list and merge neighboring chunks</li>
            </ul>
          </li>
          <li>Heap can grow heap
            <ul>
              <li>OS finds free physical pages, maps them into the address space of the requesting process, then returns the value of the end of the new heap</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Strategies
    <ul>
      <li>Best fit → search through the free list and find chunks of free memory that bigger than the request size then return one that is the smallest in that group of candidate</li>
      <li>Worst fit → case find the largest chunk and return the requested amount</li>
      <li>First fit → finds the first block that is big enough and returns the requested amount to the user
        <ul>
          <li>May pollute the beginning of the free list with a small objects</li>
          <li>address-based ordering → keeping the list ordered by the address of free space</li>
        </ul>
      </li>
      <li>Net fit → keeps an extra pointer to the location within the list where one was looking last
        <ul>
          <li>Spread the search for free space throughout the list more uniformly, thus avoiding splintering of the beginning of the list</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Segregated List
    <ul>
      <li>Slab allocator
        <ul>
          <li>The basic idea is simple: if a particular application has one (or a few) popular-sized requests that it makes, keep a separate list just to manage objects of that size</li>
        </ul>
      </li>
      <li>Slab allocator
        <ul>
          <li>Specifically, when the kernel boots up, it allocates a number of object caches for kernel objects that are likely to be requested frequently</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Buddy allocation
    <ul>
      <li>big space of size 2^N</li>
      <li>search for free space recursively divides free space by two until a block that is big enough to accommodate the request is found</li>
      <li>may suffer internal fragmentation</li>
      <li>When returning the 8KB block to the free list, the allocator checks whether the “buddy” 8KB is free; if so, it coalesces the two blocks into a 16KB block.</li>
    </ul>
  </li>
  <li>One major problem with many of the approaches described above is their lack of scaling.</li>
</ul>]]></content><author><name>JSC</name><email>newwin0198@handong.ac.kr</email></author><category term="OS" /><category term="OS" /><summary type="html"><![CDATA[OS Ch.17]]></summary></entry></feed>